---
title: Loggers | Dagster
description: Dagster includes an extensible logging system.
---

# Loggers

Dagster includes an extensible logging system that is available in the body of every solid.
Messages logged using this system will be aggregated and displayed in Dagit, providing a single
view of all the logs from your pipeline's execution.

You can write your own Dagster loggers, to customize the way log messages are formatted or stored.

You can also view the raw `stdout` and `stderr` output from your solid executions in Dagit, and
customize how these are stored.

## Relevant APIs

### Logging from solids and defining custom loggers

| Name                                                  | Description                                                                                                                                                                                          |
| ----------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| <PyObject object="logger" decorator />                | The API used to define loggers. The decorator returns a <PyObject object="LoggerDefinition" />.                                                                                                      |
| <PyObject object="SolidExecutionContext" />           | The `context` object available as the first argument to every solid's [compute function](/concepts/solids-pipelines/solids#defining-a-solid). The Dagster log manager is available at `context.log`. |
| <PyObject module="dagster" object="ModeDefinition" /> | You can use [modes](/concepts/modes-resources/modes) to vary the loggers available to a pipeline in different execution environments.                                                                |
| <PyObject object="DagsterLogManager" />               | The log manager object available within a solid's compute function as `context.log`.                                                                                                                 |
| <PyObject object="LoggerDefinition" />                | Base class for loggers. You will probably not want to initialize this class directly. Instead, use <PyObject object="logger" decorator />                                                            |

### Monitoring stdout and stderr

| Name                                                                                      | Description                                                                                                                                            |
| ----------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |
| <PyObject module="dagster.core.storage.compute_log_manager" object="ComputeLogManager" /> | The compute log manager is used by the Dagster instance to write stdout and stderr to a location accessible by Dagit.                                  |
| <PyObject object="DagsterInstance" />                                                     | The Dagster instance can be configured to provide facilities that will be available to all the pipelines running within a given deployment of Dagster. |

## Overview

Any solid can emit log messages at any point in its computation.

```python file=/concepts/logging/builtin_logger.py startafter=start_builtin_logger_marker_0 endbefore=end_builtin_logger_marker_0
@solid
def hello_logs(context):
    context.log.info("Hello, world!")
```

The log manager is available on the `context` object passed as the first argument to all solid
compute functions, as `solid.log`.

Messages may be logged at any of the ordinary
Python <a href="https://docs.python.org/3/library/logging.html">`logging`</a> levels 
(`critical()`, `error()`, `warning()`, `info()`, or `debug()`), and can be searched and filtered
by level in Dagit's integrated log viewer.

If you're unsure what level to log at, follow the
standard <a href="https://docs.python.org/3/howto/logging.html#when-to-use-logging">Python 
convention</a>.

## Logging from solids

When you run a pipeline in any mode that doesn't explicitly specify loggers, by default Dagster will
use a colored console logger that writes to stdout:

<pre>
<span style={{color: "green"}}>2021-03-16 16:03:58</span> - dagster - <span style={{color: "blue"}}>INFO</span> - system - e7073b43-55c1-4c4b-a291-650e5a4020ad - hello_logs - Hello, world!
</pre>

Logs also stream back to the Dagit frontend in real time:

<img src="/images/concepts/logging/dagit.png" />

Dagit exposes a powerful facility for filtering log messages based on execution steps and log levels:

<img src="/images/concepts/logging/dagit-filter.png" />

### Debugging with logs

What happens if we introduce an error into our solid logic?

```python file=/concepts/logging/builtin_logger.py startafter=start_builtin_logger_error_marker_0 endbefore=end_builtin_logger_error_marker_0
@solid
def hello_logs_error(context):
    raise Exception("Somebody set up us the bomb")


@pipeline
def demo_pipeline_error():
    hello_logs_error()
```

Errors in user code are caught by the Dagster machinery to ensure pipelines gracefully halt or
continue to execute, but messages including the original stack trace will get logged both to the
default colored console logger (or any other loggers configured on your pipeline's mode) and back
to Dagit.

Messages at level `ERROR` or above are highlighted both in Dagit and in the console logs, so
we can easily pick them out of logs even without filtering.

<pre>
<span style={{color: "green"}}>2021-03-16 16:18:12</span> - dagster - <span style={{color: "blue"}}>DEBUG</span> - demo_pipeline_error - 74e82ed3-3fc0-4850-b73a-5286ce344b3f - 53997 - ENGINE_EVENT - Finished steps in process (pid: 53997) in 46ms<br/>
<span style={{color: "green"}}>2021-03-16 16:18:12</span> - dagster - <span style={{color: "blue"}}>ERROR</span> - <span style={{color: "red"}}>demo_pipeline_error - 74e82ed3-3fc0-4850-b73a-5286ce344b3f - 53997 - PIPELINE_FAILURE - Execution of pipeline "demo_pipeline_error" failed. Steps failed: ['hello_logs_error'].</span></pre>

<img src="/images/concepts/logging/dagit-error.png" />

In many cases, especially for local development, this log viewer, coupled with solid reexecution,
is sufficient to enable a fast debug cycle for data pipelines.

### Configuring the built-in logger

Suppose that we've gotten the kinks out of our pipelines developing locally, and now we want to run
in production -- without all of the log spew from `DEBUG` messages that was helpful during
development.

Just like solids, loggers can be configured when you run a pipeline. For example, to filter all
messages below `ERROR` out of the colored console logger, add the following snippet to your config
YAML:

```yaml file=/concepts/logging/config.yaml
loggers:
  console:
    config:
      log_level: ERROR
```

When you execute the pipeline with this config, you'll only see logs at the `ERROR` and
`CRITICAL` levels.

## Switching loggers based on mode

Dagster and its associated libraries include a number of built-in loggers.

| Name                                                                    | Description                                                                                                  |
| ----------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------ |
| <PyObject module="dagster.loggers" object="colored_console_logger" />   | The default logger present on all pipeline modes that don't define their own loggers                         |
| <PyObject module="dagster.loggers" object="json_console_logger" />      | An alternative JSON-formatted console logger appropriate when logs are being consumed by machine (e.g., ELK) |
| <PyObject module="dagster_aws.cloudwatch" object="cloudwatch_logger" /> | A logger that sends messages to <a href="https://aws.amazon.com/cloudwatch/">AWS Cloudwatch</a>              |
| <PyObject module="dagster_papertrail" object="papertrail_logger" />     | A logger that sends messages to <a href="https://www.papertrail.com/">Papertrail</a>                         |


Logging is environment-specific: you don't want messages generated by data scientists' local
development loops to be aggregated with production messages; on the other hand, you may find that
in production console logging is irrelevant or even counterproductive.

Dagster recognizes this by attaching loggers to [modes](/concepts/modes-resources/modes) so that you can
seamlessly switch from, e.g., Cloudwatch logging in production to console logging in development
and test, without changing any of your code.

```python file=/concepts/logging/logging_modes.py startafter=start_logging_mode_marker_0 endbefore=end_logging_mode_marker_0
@solid
def hello_logs(context):
    context.log.info("Hello, world!")


@pipeline(
    mode_defs=[
        ModeDefinition(name="local", logger_defs={"console": colored_console_logger}),
        ModeDefinition(name="prod", logger_defs={"cloudwatch": cloudwatch_logger}),
    ]
)
def hello_modes():
    hello_logs()
```

From Dagit, you can switch your pipeline mode to 'prod' and edit config in order to use the new
Cloudwatch logger, for example:

```yaml file=/concepts/logging/config_modes.yaml
loggers:
  cloudwatch:
    config:
      log_level: ERROR
      log_group_name: /my/cool/cloudwatch/log/group
      log_stream_name: very_good_log_stream
```
## Defining your own loggers

You may find yourself wanting to add or supplement the built-in loggers so that Dagster logs
are integrated with the rest of your log aggregation and monitoring infrastructure.

For example, you may be operating in a containerized environment where container stdout is
aggregated by a tool such as Logstash. In this kind of environment, where logs will be aggregated
and parsed by machine, we'd much prefer for log messages to be structured as JSON, like:

```json
{"orig_message": "Hello, world!", "log_message_id": "49854579-e4d1-4289-8453-b3e177b20056", ...}
```

In fact, a logger that prints JSON-formatted single-line messages like this to the console is
already included as <PyObject module="dagster.loggers" object="json_console_logger" />. But let's look at how we might
implement a simplified version of this logger.

Loggers are defined internally using the <PyObject module="dagster" object="LoggerDefinition" displayText="LoggerDefinition" />
class, but, following a common pattern in the Dagster codebase, the <PyObject module="dagster" object="logger" decorator />
decorator exposes a simpler API for the common use case and
is typically what you'll use to define your own loggers.

The function decorated by <PyObject module="dagster" object="logger" decorator /> should take a
single argument, the `init_context` available during logger initialization
(of type <PyObject module="dagster" object="InitLoggerContext" />), and return
a <a href="https://docs.python.org/3/library/logging.html#logging.Logger">`logging.Logger`</a>.

```python file=/concepts/logging/custom_logger.py startafter=start_custom_logger_marker_0 endbefore=end_custom_logger_marker_0
@logger(
    {
        "log_level": Field(str, is_required=False, default_value="INFO"),
        "name": Field(str, is_required=False, default_value="dagster"),
    },
    description="A JSON-formatted console logger",
)
def json_console_logger(init_context):
    level = init_context.logger_config["log_level"]
    name = init_context.logger_config["name"]

    klass = logging.getLoggerClass()
    logger_ = klass(name, level=level)

    handler = logging.StreamHandler()

    class JsonFormatter(logging.Formatter):
        def format(self, record):
            return json.dumps(record.__dict__)

    handler.setFormatter(JsonFormatter())
    logger_.addHandler(handler)

    return logger_
```

Most of this should be familiar to you if you've written a Python logger before. (If not, you may
want to read the Python <a href="https://docs.python.org/3/howto/logging.html">logging tutorial</a>.)

Our custom logger

```python file=/concepts/logging/custom_logger.py startafter=start_custom_logger_marker_1 endbefore=end_custom_logger_marker_1
```


As you can see, you can specify the logger name in the run config. It also takes a `config` argument,
representing the config that users can pass to the logger, for example:

```yaml file=/concepts/logging/config_custom_logger.yaml
loggers:
  my_json_logger:
    config:
      log_level: INFO
```

When you execute the pipeline, you'll notice that you are no longer using the built-in logger but
your custom json logger instead.

<img src="/images/concepts/logging/custom-logger.png" />


## Accessing stderr and stdout 
<!-- TODO -->

## Relationship with Python logging
