
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>dagster_databricks.databricks_pyspark_step_launcher &#8212; Dagster</title>
    <link rel="stylesheet" href="../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../_static/language_data.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
 
<link rel="stylesheet" href="../../_static/custom.css" type="text/css" />


<meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
    <div class="documentwrapper">
        <div class="bodywrapper">
            <div class="related top">
                &nbsp;
<nav id="rellinks">
    <ul>
        <li>
            <a href="/" title="Home">Home</a>
        </li>
    </ul>
</nav>
            </div>
            

            

            <div class="body" role="main">
                
  <h1>Source code for dagster_databricks.databricks_pyspark_step_launcher</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">io</span>
<span class="kn">import</span> <span class="nn">os.path</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">tempfile</span>

<span class="kn">from</span> <span class="nn">dagster</span> <span class="kn">import</span> <span class="n">Bool</span><span class="p">,</span> <span class="n">Field</span><span class="p">,</span> <span class="n">StringSource</span><span class="p">,</span> <span class="n">check</span><span class="p">,</span> <span class="n">resource</span>
<span class="kn">from</span> <span class="nn">dagster.core.definitions.step_launcher</span> <span class="kn">import</span> <span class="n">StepLauncher</span>
<span class="kn">from</span> <span class="nn">dagster.core.errors</span> <span class="kn">import</span> <span class="n">raise_execution_interrupts</span>
<span class="kn">from</span> <span class="nn">dagster.core.events</span> <span class="kn">import</span> <span class="n">log_step_event</span>
<span class="kn">from</span> <span class="nn">dagster.core.execution.plan.external_step</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">PICKLED_EVENTS_FILE_NAME</span><span class="p">,</span>
    <span class="n">PICKLED_STEP_RUN_REF_FILE_NAME</span><span class="p">,</span>
    <span class="n">step_context_to_step_run_ref</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">dagster.serdes</span> <span class="kn">import</span> <span class="n">deserialize_value</span>
<span class="kn">from</span> <span class="nn">dagster_databricks</span> <span class="kn">import</span> <span class="n">DatabricksJobRunner</span><span class="p">,</span> <span class="n">databricks_step_main</span>
<span class="kn">from</span> <span class="nn">dagster_pyspark.utils</span> <span class="kn">import</span> <span class="n">build_pyspark_zip</span>

<span class="kn">from</span> <span class="nn">.configs</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">define_databricks_secrets_config</span><span class="p">,</span>
    <span class="n">define_databricks_storage_config</span><span class="p">,</span>
    <span class="n">define_databricks_submit_run_config</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">CODE_ZIP_NAME</span> <span class="o">=</span> <span class="s2">&quot;code.zip&quot;</span>
<span class="n">PICKLED_CONFIG_FILE_NAME</span> <span class="o">=</span> <span class="s2">&quot;config.pkl&quot;</span>


<div class="viewcode-block" id="databricks_pyspark_step_launcher"><a class="viewcode-back" href="../../sections/api/apidocs/libraries/dagster_databricks.html#dagster_databricks.databricks_pyspark_step_launcher">[docs]</a><span class="nd">@resource</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;run_config&quot;</span><span class="p">:</span> <span class="n">define_databricks_submit_run_config</span><span class="p">(),</span>
        <span class="s2">&quot;databricks_host&quot;</span><span class="p">:</span> <span class="n">Field</span><span class="p">(</span>
            <span class="n">StringSource</span><span class="p">,</span>
            <span class="n">is_required</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Databricks host, e.g. uksouth.azuredatabricks.com&quot;</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="s2">&quot;databricks_token&quot;</span><span class="p">:</span> <span class="n">Field</span><span class="p">(</span>
            <span class="n">StringSource</span><span class="p">,</span> <span class="n">is_required</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Databricks access token&quot;</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="s2">&quot;secrets_to_env_variables&quot;</span><span class="p">:</span> <span class="n">define_databricks_secrets_config</span><span class="p">(),</span>
        <span class="s2">&quot;storage&quot;</span><span class="p">:</span> <span class="n">define_databricks_storage_config</span><span class="p">(),</span>
        <span class="s2">&quot;local_pipeline_package_path&quot;</span><span class="p">:</span> <span class="n">Field</span><span class="p">(</span>
            <span class="n">StringSource</span><span class="p">,</span>
            <span class="n">is_required</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Absolute path to the package that contains the pipeline definition(s) &quot;</span>
            <span class="s2">&quot;whose steps will execute remotely on Databricks. This is a path on the local &quot;</span>
            <span class="s2">&quot;fileystem of the process executing the pipeline. Before every step run, the &quot;</span>
            <span class="s2">&quot;launcher will zip up the code in this path, upload it to DBFS, and unzip it &quot;</span>
            <span class="s2">&quot;into the Python path of the remote Spark process. This gives the remote process &quot;</span>
            <span class="s2">&quot;access to up-to-date user code.&quot;</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="s2">&quot;staging_prefix&quot;</span><span class="p">:</span> <span class="n">Field</span><span class="p">(</span>
            <span class="n">StringSource</span><span class="p">,</span>
            <span class="n">is_required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">default_value</span><span class="o">=</span><span class="s2">&quot;/dagster_staging&quot;</span><span class="p">,</span>
            <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Directory in DBFS to use for uploaded job code. Must be absolute.&quot;</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="s2">&quot;wait_for_logs&quot;</span><span class="p">:</span> <span class="n">Field</span><span class="p">(</span>
            <span class="n">Bool</span><span class="p">,</span>
            <span class="n">is_required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">default_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">description</span><span class="o">=</span><span class="s2">&quot;If set, and if the specified cluster is configured to export logs, &quot;</span>
            <span class="s2">&quot;the system will wait after job completion for the logs to appear in the configured &quot;</span>
            <span class="s2">&quot;location. Note that logs are copied every 5 minutes, so enabling this will add &quot;</span>
            <span class="s2">&quot;several minutes to the job runtime.&quot;</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">}</span>
<span class="p">)</span>
<span class="k">def</span> <span class="nf">databricks_pyspark_step_launcher</span><span class="p">(</span><span class="n">context</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Resource for running solids as a Databricks Job.</span>

<span class="sd">    When this resource is used, the solid will be executed in Databricks using the &#39;Run Submit&#39;</span>
<span class="sd">    API. Pipeline code will be zipped up and copied to a directory in DBFS along with the solid&#39;s</span>
<span class="sd">    execution context.</span>

<span class="sd">    Use the &#39;run_config&#39; configuration to specify the details of the Databricks cluster used, and</span>
<span class="sd">    the &#39;storage&#39; key to configure persistent storage on that cluster. Storage is accessed by</span>
<span class="sd">    setting the credentials in the Spark context, as documented `here for S3`_ and `here for ADLS`_.</span>

<span class="sd">    .. _`here for S3`: https://docs.databricks.com/data/data-sources/aws/amazon-s3.html#alternative-1-set-aws-keys-in-the-spark-context</span>
<span class="sd">    .. _`here for ADLS`: https://docs.microsoft.com/en-gb/azure/databricks/data/data-sources/azure/azure-datalake-gen2#--access-directly-using-the-storage-account-access-key</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">DatabricksPySparkStepLauncher</span><span class="p">(</span><span class="o">**</span><span class="n">context</span><span class="o">.</span><span class="n">resource_config</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">DatabricksPySparkStepLauncher</span><span class="p">(</span><span class="n">StepLauncher</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">run_config</span><span class="p">,</span>
        <span class="n">databricks_host</span><span class="p">,</span>
        <span class="n">databricks_token</span><span class="p">,</span>
        <span class="n">secrets_to_env_variables</span><span class="p">,</span>
        <span class="n">storage</span><span class="p">,</span>
        <span class="n">local_pipeline_package_path</span><span class="p">,</span>
        <span class="n">staging_prefix</span><span class="p">,</span>
        <span class="n">wait_for_logs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">run_config</span> <span class="o">=</span> <span class="n">check</span><span class="o">.</span><span class="n">dict_param</span><span class="p">(</span><span class="n">run_config</span><span class="p">,</span> <span class="s2">&quot;run_config&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">databricks_host</span> <span class="o">=</span> <span class="n">check</span><span class="o">.</span><span class="n">str_param</span><span class="p">(</span><span class="n">databricks_host</span><span class="p">,</span> <span class="s2">&quot;databricks_host&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">databricks_token</span> <span class="o">=</span> <span class="n">check</span><span class="o">.</span><span class="n">str_param</span><span class="p">(</span><span class="n">databricks_token</span><span class="p">,</span> <span class="s2">&quot;databricks_token&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">secrets</span> <span class="o">=</span> <span class="n">check</span><span class="o">.</span><span class="n">list_param</span><span class="p">(</span><span class="n">secrets_to_env_variables</span><span class="p">,</span> <span class="s2">&quot;secrets_to_env_variables&quot;</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">storage</span> <span class="o">=</span> <span class="n">check</span><span class="o">.</span><span class="n">dict_param</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="s2">&quot;storage&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_pipeline_package_path</span> <span class="o">=</span> <span class="n">check</span><span class="o">.</span><span class="n">str_param</span><span class="p">(</span>
            <span class="n">local_pipeline_package_path</span><span class="p">,</span> <span class="s2">&quot;local_pipeline_package_path&quot;</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">staging_prefix</span> <span class="o">=</span> <span class="n">check</span><span class="o">.</span><span class="n">str_param</span><span class="p">(</span><span class="n">staging_prefix</span><span class="p">,</span> <span class="s2">&quot;staging_prefix&quot;</span><span class="p">)</span>
        <span class="n">check</span><span class="o">.</span><span class="n">invariant</span><span class="p">(</span><span class="n">staging_prefix</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">),</span> <span class="s2">&quot;staging_prefix must be an absolute path&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wait_for_logs</span> <span class="o">=</span> <span class="n">check</span><span class="o">.</span><span class="n">bool_param</span><span class="p">(</span><span class="n">wait_for_logs</span><span class="p">,</span> <span class="s2">&quot;wait_for_logs&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">databricks_runner</span> <span class="o">=</span> <span class="n">DatabricksJobRunner</span><span class="p">(</span><span class="n">host</span><span class="o">=</span><span class="n">databricks_host</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="n">databricks_token</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">launch_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">step_context</span><span class="p">,</span> <span class="n">prior_attempts_count</span><span class="p">):</span>
        <span class="n">step_run_ref</span> <span class="o">=</span> <span class="n">step_context_to_step_run_ref</span><span class="p">(</span>
            <span class="n">step_context</span><span class="p">,</span> <span class="n">prior_attempts_count</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_pipeline_package_path</span>
        <span class="p">)</span>
        <span class="n">run_id</span> <span class="o">=</span> <span class="n">step_context</span><span class="o">.</span><span class="n">pipeline_run</span><span class="o">.</span><span class="n">run_id</span>
        <span class="n">log</span> <span class="o">=</span> <span class="n">step_context</span><span class="o">.</span><span class="n">log</span>

        <span class="n">step_key</span> <span class="o">=</span> <span class="n">step_run_ref</span><span class="o">.</span><span class="n">step_key</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_upload_artifacts</span><span class="p">(</span><span class="n">log</span><span class="p">,</span> <span class="n">step_run_ref</span><span class="p">,</span> <span class="n">run_id</span><span class="p">,</span> <span class="n">step_key</span><span class="p">)</span>

        <span class="n">task</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_databricks_task</span><span class="p">(</span><span class="n">run_id</span><span class="p">,</span> <span class="n">step_key</span><span class="p">)</span>
        <span class="n">databricks_run_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">databricks_runner</span><span class="o">.</span><span class="n">submit_run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">run_config</span><span class="p">,</span> <span class="n">task</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># If this is being called within a `capture_interrupts` context, allow interrupts while</span>
            <span class="c1"># waiting for the  execution to complete, so that we can terminate slow or hanging steps</span>
            <span class="k">with</span> <span class="n">raise_execution_interrupts</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">databricks_runner</span><span class="o">.</span><span class="n">wait_for_run_to_complete</span><span class="p">(</span><span class="n">log</span><span class="p">,</span> <span class="n">databricks_run_id</span><span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">wait_for_logs</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_log_logs_from_cluster</span><span class="p">(</span><span class="n">log</span><span class="p">,</span> <span class="n">databricks_run_id</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_step_events</span><span class="p">(</span><span class="n">run_id</span><span class="p">,</span> <span class="n">step_key</span><span class="p">):</span>
            <span class="n">log_step_event</span><span class="p">(</span><span class="n">step_context</span><span class="p">,</span> <span class="n">event</span><span class="p">)</span>
            <span class="k">yield</span> <span class="n">event</span>

    <span class="k">def</span> <span class="nf">get_step_events</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">run_id</span><span class="p">,</span> <span class="n">step_key</span><span class="p">):</span>
        <span class="n">path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dbfs_path</span><span class="p">(</span><span class="n">run_id</span><span class="p">,</span> <span class="n">step_key</span><span class="p">,</span> <span class="n">PICKLED_EVENTS_FILE_NAME</span><span class="p">)</span>
        <span class="n">events_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">databricks_runner</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">read_file</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">deserialize_value</span><span class="p">(</span><span class="n">pickle</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">events_data</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_get_databricks_task</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">run_id</span><span class="p">,</span> <span class="n">step_key</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Construct the &#39;task&#39; parameter to  be submitted to the Databricks API.</span>

<span class="sd">        This will create a &#39;spark_python_task&#39; dict where `python_file` is a path on DBFS</span>
<span class="sd">        pointing to the &#39;databricks_step_main.py&#39; file, and `parameters` is an array with a single</span>
<span class="sd">        element, a path on DBFS pointing to the picked `step_run_ref` data.</span>

<span class="sd">        See https://docs.databricks.com/dev-tools/api/latest/jobs.html#jobssparkpythontask.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">python_file</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dbfs_path</span><span class="p">(</span><span class="n">run_id</span><span class="p">,</span> <span class="n">step_key</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_main_file_name</span><span class="p">())</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_internal_dbfs_path</span><span class="p">(</span><span class="n">run_id</span><span class="p">,</span> <span class="n">step_key</span><span class="p">,</span> <span class="n">PICKLED_STEP_RUN_REF_FILE_NAME</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_internal_dbfs_path</span><span class="p">(</span><span class="n">run_id</span><span class="p">,</span> <span class="n">step_key</span><span class="p">,</span> <span class="n">PICKLED_CONFIG_FILE_NAME</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_internal_dbfs_path</span><span class="p">(</span><span class="n">run_id</span><span class="p">,</span> <span class="n">step_key</span><span class="p">,</span> <span class="n">CODE_ZIP_NAME</span><span class="p">),</span>
        <span class="p">]</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;spark_python_task&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;python_file&quot;</span><span class="p">:</span> <span class="n">python_file</span><span class="p">,</span> <span class="s2">&quot;parameters&quot;</span><span class="p">:</span> <span class="n">parameters</span><span class="p">}}</span>

    <span class="k">def</span> <span class="nf">_upload_artifacts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log</span><span class="p">,</span> <span class="n">step_run_ref</span><span class="p">,</span> <span class="n">run_id</span><span class="p">,</span> <span class="n">step_key</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Upload the step run ref and pyspark code to DBFS to run as a job.&quot;&quot;&quot;</span>

        <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Uploading main file to DBFS&quot;</span><span class="p">)</span>
        <span class="n">main_local_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_main_file_local_path</span><span class="p">()</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">main_local_path</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">infile</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">databricks_runner</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">put_file</span><span class="p">(</span>
                <span class="n">infile</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dbfs_path</span><span class="p">(</span><span class="n">run_id</span><span class="p">,</span> <span class="n">step_key</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_main_file_name</span><span class="p">())</span>
            <span class="p">)</span>

        <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Uploading pipeline to DBFS&quot;</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">TemporaryDirectory</span><span class="p">()</span> <span class="k">as</span> <span class="n">temp_dir</span><span class="p">:</span>
            <span class="c1"># Zip and upload package containing pipeline</span>
            <span class="n">zip_local_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">temp_dir</span><span class="p">,</span> <span class="n">CODE_ZIP_NAME</span><span class="p">)</span>
            <span class="n">build_pyspark_zip</span><span class="p">(</span><span class="n">zip_local_path</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_pipeline_package_path</span><span class="p">)</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">zip_local_path</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">infile</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">databricks_runner</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">put_file</span><span class="p">(</span>
                    <span class="n">infile</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dbfs_path</span><span class="p">(</span><span class="n">run_id</span><span class="p">,</span> <span class="n">step_key</span><span class="p">,</span> <span class="n">CODE_ZIP_NAME</span><span class="p">)</span>
                <span class="p">)</span>

        <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Uploading step run ref file to DBFS&quot;</span><span class="p">)</span>
        <span class="n">step_pickle_file</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">()</span>

        <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">step_run_ref</span><span class="p">,</span> <span class="n">step_pickle_file</span><span class="p">)</span>
        <span class="n">step_pickle_file</span><span class="o">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">databricks_runner</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">put_file</span><span class="p">(</span>
            <span class="n">step_pickle_file</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dbfs_path</span><span class="p">(</span><span class="n">run_id</span><span class="p">,</span> <span class="n">step_key</span><span class="p">,</span> <span class="n">PICKLED_STEP_RUN_REF_FILE_NAME</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="n">databricks_config</span> <span class="o">=</span> <span class="n">DatabricksConfig</span><span class="p">(</span><span class="n">storage</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">storage</span><span class="p">,</span> <span class="n">secrets</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">secrets</span><span class="p">,)</span>
        <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Uploading Databricks configuration to DBFS&quot;</span><span class="p">)</span>
        <span class="n">databricks_config_file</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">()</span>

        <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">databricks_config</span><span class="p">,</span> <span class="n">databricks_config_file</span><span class="p">)</span>
        <span class="n">databricks_config_file</span><span class="o">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">databricks_runner</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">put_file</span><span class="p">(</span>
            <span class="n">databricks_config_file</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dbfs_path</span><span class="p">(</span><span class="n">run_id</span><span class="p">,</span> <span class="n">step_key</span><span class="p">,</span> <span class="n">PICKLED_CONFIG_FILE_NAME</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_log_logs_from_cluster</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log</span><span class="p">,</span> <span class="n">run_id</span><span class="p">):</span>
        <span class="n">logs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">databricks_runner</span><span class="o">.</span><span class="n">retrieve_logs_for_run_id</span><span class="p">(</span><span class="n">log</span><span class="p">,</span> <span class="n">run_id</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">logs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">stdout</span><span class="p">,</span> <span class="n">stderr</span> <span class="o">=</span> <span class="n">logs</span>
        <span class="k">if</span> <span class="n">stderr</span><span class="p">:</span>
            <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="n">stderr</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">stdout</span><span class="p">:</span>
            <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="n">stdout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_main_file_name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_main_file_local_path</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">_main_file_local_path</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">databricks_step_main</span><span class="o">.</span><span class="vm">__file__</span>

    <span class="k">def</span> <span class="nf">_dbfs_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">run_id</span><span class="p">,</span> <span class="n">step_key</span><span class="p">,</span> <span class="n">filename</span><span class="p">):</span>
        <span class="n">path</span> <span class="o">=</span> <span class="s2">&quot;/&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">staging_prefix</span><span class="p">,</span> <span class="n">run_id</span><span class="p">,</span> <span class="n">step_key</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">filename</span><span class="p">)])</span>
        <span class="k">return</span> <span class="s2">&quot;dbfs://</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_internal_dbfs_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">run_id</span><span class="p">,</span> <span class="n">step_key</span><span class="p">,</span> <span class="n">filename</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Scripts running on Databricks should access DBFS at /dbfs/.&quot;&quot;&quot;</span>
        <span class="n">path</span> <span class="o">=</span> <span class="s2">&quot;/&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">staging_prefix</span><span class="p">,</span> <span class="n">run_id</span><span class="p">,</span> <span class="n">step_key</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">filename</span><span class="p">)])</span>
        <span class="k">return</span> <span class="s2">&quot;/dbfs/</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">DatabricksConfig</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Represents configuration required by Databricks to run jobs.</span>

<span class="sd">    Instances of this class will be created when a Databricks step is launched and will contain</span>
<span class="sd">    all configuration and secrets required to set up storage and environment variables within</span>
<span class="sd">    the Databricks environment. The instance will be serialized and uploaded to Databricks</span>
<span class="sd">    by the step launcher, then deserialized as part of the &#39;main&#39; script when the job is running</span>
<span class="sd">    in Databricks.</span>

<span class="sd">    The `setup` method handles the actual setup prior to solid execution on the Databricks side.</span>

<span class="sd">    This config is separated out from the regular Dagster run config system because the setup</span>
<span class="sd">    is done by the &#39;main&#39; script before entering a Dagster context (i.e. using `run_step_from_ref`).</span>
<span class="sd">    We use a separate class to avoid coupling the setup to the format of the `step_run_ref` object.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">,</span> <span class="n">secrets</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Create a new DatabricksConfig object.</span>

<span class="sd">        `storage` and `secrets` should be of the same shape as the `storage` and</span>
<span class="sd">        `secrets_to_env_variables` config passed to `databricks_pyspark_step_launcher`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">storage</span> <span class="o">=</span> <span class="n">storage</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">secrets</span> <span class="o">=</span> <span class="n">secrets</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dbutils</span><span class="p">,</span> <span class="n">sc</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Set up storage and environment variables on Databricks.</span>

<span class="sd">        The `dbutils` and `sc` arguments must be passed in by the &#39;main&#39; script, as they</span>
<span class="sd">        aren&#39;t accessible by any other modules.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setup_storage</span><span class="p">(</span><span class="n">dbutils</span><span class="p">,</span> <span class="n">sc</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setup_environment</span><span class="p">(</span><span class="n">dbutils</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">setup_storage</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dbutils</span><span class="p">,</span> <span class="n">sc</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Set up storage using either S3 or ADLS2.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="s2">&quot;s3&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">storage</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">setup_s3_storage</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">storage</span><span class="p">[</span><span class="s2">&quot;s3&quot;</span><span class="p">],</span> <span class="n">dbutils</span><span class="p">,</span> <span class="n">sc</span><span class="p">)</span>
        <span class="k">elif</span> <span class="s2">&quot;adls2&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">storage</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">setup_adls2_storage</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">storage</span><span class="p">[</span><span class="s2">&quot;adls2&quot;</span><span class="p">],</span> <span class="n">dbutils</span><span class="p">,</span> <span class="n">sc</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;No valid storage found in Databricks configuration!&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">setup_s3_storage</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s3_storage</span><span class="p">,</span> <span class="n">dbutils</span><span class="p">,</span> <span class="n">sc</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Obtain AWS credentials from Databricks secrets and export so both Spark and boto can use them.&quot;&quot;&quot;</span>

        <span class="n">scope</span> <span class="o">=</span> <span class="n">s3_storage</span><span class="p">[</span><span class="s2">&quot;secret_scope&quot;</span><span class="p">]</span>

        <span class="n">access_key</span> <span class="o">=</span> <span class="n">dbutils</span><span class="o">.</span><span class="n">secrets</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="n">scope</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">s3_storage</span><span class="p">[</span><span class="s2">&quot;access_key_key&quot;</span><span class="p">])</span>
        <span class="n">secret_key</span> <span class="o">=</span> <span class="n">dbutils</span><span class="o">.</span><span class="n">secrets</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="n">scope</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">s3_storage</span><span class="p">[</span><span class="s2">&quot;secret_key_key&quot;</span><span class="p">])</span>

        <span class="c1"># Spark APIs will use this.</span>
        <span class="c1"># See https://docs.databricks.com/data/data-sources/aws/amazon-s3.html#alternative-1-set-aws-keys-in-the-spark-context.</span>
        <span class="n">sc</span><span class="o">.</span><span class="n">_jsc</span><span class="o">.</span><span class="n">hadoopConfiguration</span><span class="p">()</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>  <span class="c1"># pylint: disable=protected-access</span>
            <span class="s2">&quot;fs.s3n.awsAccessKeyId&quot;</span><span class="p">,</span> <span class="n">access_key</span>
        <span class="p">)</span>
        <span class="n">sc</span><span class="o">.</span><span class="n">_jsc</span><span class="o">.</span><span class="n">hadoopConfiguration</span><span class="p">()</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>  <span class="c1"># pylint: disable=protected-access</span>
            <span class="s2">&quot;fs.s3n.awsSecretAccessKey&quot;</span><span class="p">,</span> <span class="n">secret_key</span>
        <span class="p">)</span>

        <span class="c1"># Boto will use these.</span>
        <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;AWS_ACCESS_KEY_ID&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">access_key</span>
        <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;AWS_SECRET_ACCESS_KEY&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">secret_key</span>

    <span class="k">def</span> <span class="nf">setup_adls2_storage</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adls2_storage</span><span class="p">,</span> <span class="n">dbutils</span><span class="p">,</span> <span class="n">sc</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Obtain an Azure Storage Account key from Databricks secrets and export so Spark can use it.&quot;&quot;&quot;</span>
        <span class="n">storage_account_key</span> <span class="o">=</span> <span class="n">dbutils</span><span class="o">.</span><span class="n">secrets</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="n">scope</span><span class="o">=</span><span class="n">adls2_storage</span><span class="p">[</span><span class="s2">&quot;secret_scope&quot;</span><span class="p">],</span> <span class="n">key</span><span class="o">=</span><span class="n">adls2_storage</span><span class="p">[</span><span class="s2">&quot;storage_account_key_key&quot;</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="c1"># Spark APIs will use this.</span>
        <span class="c1"># See https://docs.microsoft.com/en-gb/azure/databricks/data/data-sources/azure/azure-datalake-gen2#--access-directly-using-the-storage-account-access-key</span>
        <span class="c1"># sc is globally defined in the Databricks runtime and points to the Spark context</span>
        <span class="n">sc</span><span class="o">.</span><span class="n">_jsc</span><span class="o">.</span><span class="n">hadoopConfiguration</span><span class="p">()</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>  <span class="c1"># pylint: disable=protected-access</span>
            <span class="s2">&quot;fs.azure.account.key.</span><span class="si">{}</span><span class="s2">.dfs.core.windows.net&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">adls2_storage</span><span class="p">[</span><span class="s2">&quot;storage_account_name&quot;</span><span class="p">]</span>
            <span class="p">),</span>
            <span class="n">storage_account_key</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">setup_environment</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dbutils</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Setup any environment variables required by the run.</span>

<span class="sd">        Extract any secrets in the run config and export them as environment variables.</span>

<span class="sd">        This is important for any `StringSource` config since the environment variables</span>
<span class="sd">        won&#39;t ordinarily be available in the Databricks execution environment.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">secret</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">secrets</span><span class="p">:</span>
            <span class="n">name</span> <span class="o">=</span> <span class="n">secret</span><span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">]</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">secret</span><span class="p">[</span><span class="s2">&quot;key&quot;</span><span class="p">]</span>
            <span class="n">scope</span> <span class="o">=</span> <span class="n">secret</span><span class="p">[</span><span class="s2">&quot;scope&quot;</span><span class="p">]</span>
            <span class="nb">print</span><span class="p">(</span>  <span class="c1"># pylint: disable=print-call</span>
                <span class="s2">&quot;Exporting </span><span class="si">{}</span><span class="s2"> from Databricks secret </span><span class="si">{}</span><span class="s2">, scope </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">scope</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">val</span> <span class="o">=</span> <span class="n">dbutils</span><span class="o">.</span><span class="n">secrets</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="n">scope</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">key</span><span class="p">)</span>
            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span>
</pre></div>

            </div>
            <div class="related bottom">
                &nbsp;
<nav id="rellinks">
    <ul>
        <li>
            <a href="/" title="Home">Home</a>
        </li>
    </ul>
</nav>
            </div>
            
        </div>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><h3><a href="../../index.html">Dagster</a></h3>

<div id="searchbox" style="display: none" role="search">
  <h2 id="searchlabel">Search</h2>
  <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
  </div>
</div>
<script type="text/javascript">
  $("#searchbox").show(0);
</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>


  </body>
</html>