{"parents": [{"link": "../../", "title": "Module code"}], "title": "dagster_databricks.databricks_pyspark_step_launcher", "body": "<h1>Source code for dagster_databricks.databricks_pyspark_step_launcher</h1><div class=\"highlight\"><pre>\n<span></span><span class=\"kn\">import</span> <span class=\"nn\">io</span>\n<span class=\"kn\">import</span> <span class=\"nn\">os.path</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pickle</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">dagster</span> <span class=\"kn\">import</span> <span class=\"n\">Bool</span><span class=\"p\">,</span> <span class=\"n\">Field</span><span class=\"p\">,</span> <span class=\"n\">StringSource</span><span class=\"p\">,</span> <span class=\"n\">check</span><span class=\"p\">,</span> <span class=\"n\">resource</span><span class=\"p\">,</span> <span class=\"n\">seven</span>\n<span class=\"kn\">from</span> <span class=\"nn\">dagster.core.definitions.step_launcher</span> <span class=\"kn\">import</span> <span class=\"n\">StepLauncher</span>\n<span class=\"kn\">from</span> <span class=\"nn\">dagster.core.events</span> <span class=\"kn\">import</span> <span class=\"n\">log_step_event</span>\n<span class=\"kn\">from</span> <span class=\"nn\">dagster.core.execution.plan.external_step</span> <span class=\"kn\">import</span> <span class=\"p\">(</span>\n    <span class=\"n\">PICKLED_EVENTS_FILE_NAME</span><span class=\"p\">,</span>\n    <span class=\"n\">PICKLED_STEP_RUN_REF_FILE_NAME</span><span class=\"p\">,</span>\n    <span class=\"n\">step_context_to_step_run_ref</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n<span class=\"kn\">from</span> <span class=\"nn\">dagster.serdes</span> <span class=\"kn\">import</span> <span class=\"n\">deserialize_value</span>\n<span class=\"kn\">from</span> <span class=\"nn\">dagster.utils</span> <span class=\"kn\">import</span> <span class=\"n\">raise_interrupts_immediately</span>\n<span class=\"kn\">from</span> <span class=\"nn\">dagster_databricks</span> <span class=\"kn\">import</span> <span class=\"n\">DatabricksJobRunner</span><span class=\"p\">,</span> <span class=\"n\">databricks_step_main</span>\n<span class=\"kn\">from</span> <span class=\"nn\">dagster_pyspark.utils</span> <span class=\"kn\">import</span> <span class=\"n\">build_pyspark_zip</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">.configs</span> <span class=\"kn\">import</span> <span class=\"p\">(</span>\n    <span class=\"n\">define_databricks_secrets_config</span><span class=\"p\">,</span>\n    <span class=\"n\">define_databricks_storage_config</span><span class=\"p\">,</span>\n    <span class=\"n\">define_databricks_submit_run_config</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">CODE_ZIP_NAME</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;code.zip&quot;</span>\n<span class=\"n\">PICKLED_CONFIG_FILE_NAME</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;config.pkl&quot;</span>\n\n\n<div class=\"viewcode-block\" id=\"databricks_pyspark_step_launcher\"><a class=\"viewcode-back\" href=\"../../../sections/api/apidocs/libraries/dagster_databricks/#dagster_databricks.databricks_pyspark_step_launcher\">[docs]</a><span class=\"nd\">@resource</span><span class=\"p\">(</span>\n    <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;run_config&quot;</span><span class=\"p\">:</span> <span class=\"n\">define_databricks_submit_run_config</span><span class=\"p\">(),</span>\n        <span class=\"s2\">&quot;databricks_host&quot;</span><span class=\"p\">:</span> <span class=\"n\">Field</span><span class=\"p\">(</span>\n            <span class=\"n\">StringSource</span><span class=\"p\">,</span>\n            <span class=\"n\">is_required</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>\n            <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;Databricks host, e.g. uksouth.azuredatabricks.com&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">),</span>\n        <span class=\"s2\">&quot;databricks_token&quot;</span><span class=\"p\">:</span> <span class=\"n\">Field</span><span class=\"p\">(</span>\n            <span class=\"n\">StringSource</span><span class=\"p\">,</span> <span class=\"n\">is_required</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;Databricks access token&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">),</span>\n        <span class=\"s2\">&quot;secrets_to_env_variables&quot;</span><span class=\"p\">:</span> <span class=\"n\">define_databricks_secrets_config</span><span class=\"p\">(),</span>\n        <span class=\"s2\">&quot;storage&quot;</span><span class=\"p\">:</span> <span class=\"n\">define_databricks_storage_config</span><span class=\"p\">(),</span>\n        <span class=\"s2\">&quot;local_pipeline_package_path&quot;</span><span class=\"p\">:</span> <span class=\"n\">Field</span><span class=\"p\">(</span>\n            <span class=\"n\">StringSource</span><span class=\"p\">,</span>\n            <span class=\"n\">is_required</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>\n            <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;Absolute path to the package that contains the pipeline definition(s) &quot;</span>\n            <span class=\"s2\">&quot;whose steps will execute remotely on Databricks. This is a path on the local &quot;</span>\n            <span class=\"s2\">&quot;fileystem of the process executing the pipeline. Before every step run, the &quot;</span>\n            <span class=\"s2\">&quot;launcher will zip up the code in this path, upload it to DBFS, and unzip it &quot;</span>\n            <span class=\"s2\">&quot;into the Python path of the remote Spark process. This gives the remote process &quot;</span>\n            <span class=\"s2\">&quot;access to up-to-date user code.&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">),</span>\n        <span class=\"s2\">&quot;staging_prefix&quot;</span><span class=\"p\">:</span> <span class=\"n\">Field</span><span class=\"p\">(</span>\n            <span class=\"n\">StringSource</span><span class=\"p\">,</span>\n            <span class=\"n\">is_required</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span>\n            <span class=\"n\">default_value</span><span class=\"o\">=</span><span class=\"s2\">&quot;/dagster_staging&quot;</span><span class=\"p\">,</span>\n            <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;Directory in DBFS to use for uploaded job code. Must be absolute.&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">),</span>\n        <span class=\"s2\">&quot;wait_for_logs&quot;</span><span class=\"p\">:</span> <span class=\"n\">Field</span><span class=\"p\">(</span>\n            <span class=\"n\">Bool</span><span class=\"p\">,</span>\n            <span class=\"n\">is_required</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span>\n            <span class=\"n\">default_value</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span>\n            <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;If set, and if the specified cluster is configured to export logs, &quot;</span>\n            <span class=\"s2\">&quot;the system will wait after job completion for the logs to appear in the configured &quot;</span>\n            <span class=\"s2\">&quot;location. Note that logs are copied every 5 minutes, so enabling this will add &quot;</span>\n            <span class=\"s2\">&quot;several minutes to the job runtime.&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">),</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">)</span>\n<span class=\"k\">def</span> <span class=\"nf\">databricks_pyspark_step_launcher</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">):</span>\n    <span class=\"sd\">&quot;&quot;&quot;Resource for running solids as a Databricks Job.</span>\n\n<span class=\"sd\">    When this resource is used, the solid will be executed in Databricks using the &#39;Run Submit&#39;</span>\n<span class=\"sd\">    API. Pipeline code will be zipped up and copied to a directory in DBFS along with the solid&#39;s</span>\n<span class=\"sd\">    execution context.</span>\n\n<span class=\"sd\">    Use the &#39;run_config&#39; configuration to specify the details of the Databricks cluster used, and</span>\n<span class=\"sd\">    the &#39;storage&#39; key to configure persistent storage on that cluster. Storage is accessed by</span>\n<span class=\"sd\">    setting the credentials in the Spark context, as documented `here for S3`_ and `here for ADLS`_.</span>\n\n<span class=\"sd\">    .. _`here for S3`: https://docs.databricks.com/data/data-sources/aws/amazon-s3.html#alternative-1-set-aws-keys-in-the-spark-context</span>\n<span class=\"sd\">    .. _`here for ADLS`: https://docs.microsoft.com/en-gb/azure/databricks/data/data-sources/azure/azure-datalake-gen2#--access-directly-using-the-storage-account-access-key</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"k\">return</span> <span class=\"n\">DatabricksPySparkStepLauncher</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">resource_config</span><span class=\"p\">)</span></div>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">DatabricksPySparkStepLauncher</span><span class=\"p\">(</span><span class=\"n\">StepLauncher</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span>\n        <span class=\"bp\">self</span><span class=\"p\">,</span>\n        <span class=\"n\">run_config</span><span class=\"p\">,</span>\n        <span class=\"n\">databricks_host</span><span class=\"p\">,</span>\n        <span class=\"n\">databricks_token</span><span class=\"p\">,</span>\n        <span class=\"n\">secrets_to_env_variables</span><span class=\"p\">,</span>\n        <span class=\"n\">storage</span><span class=\"p\">,</span>\n        <span class=\"n\">local_pipeline_package_path</span><span class=\"p\">,</span>\n        <span class=\"n\">staging_prefix</span><span class=\"p\">,</span>\n        <span class=\"n\">wait_for_logs</span><span class=\"p\">,</span>\n    <span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">run_config</span> <span class=\"o\">=</span> <span class=\"n\">check</span><span class=\"o\">.</span><span class=\"n\">dict_param</span><span class=\"p\">(</span><span class=\"n\">run_config</span><span class=\"p\">,</span> <span class=\"s2\">&quot;run_config&quot;</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">databricks_host</span> <span class=\"o\">=</span> <span class=\"n\">check</span><span class=\"o\">.</span><span class=\"n\">str_param</span><span class=\"p\">(</span><span class=\"n\">databricks_host</span><span class=\"p\">,</span> <span class=\"s2\">&quot;databricks_host&quot;</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">databricks_token</span> <span class=\"o\">=</span> <span class=\"n\">check</span><span class=\"o\">.</span><span class=\"n\">str_param</span><span class=\"p\">(</span><span class=\"n\">databricks_token</span><span class=\"p\">,</span> <span class=\"s2\">&quot;databricks_token&quot;</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">secrets</span> <span class=\"o\">=</span> <span class=\"n\">check</span><span class=\"o\">.</span><span class=\"n\">list_param</span><span class=\"p\">(</span><span class=\"n\">secrets_to_env_variables</span><span class=\"p\">,</span> <span class=\"s2\">&quot;secrets_to_env_variables&quot;</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">storage</span> <span class=\"o\">=</span> <span class=\"n\">check</span><span class=\"o\">.</span><span class=\"n\">dict_param</span><span class=\"p\">(</span><span class=\"n\">storage</span><span class=\"p\">,</span> <span class=\"s2\">&quot;storage&quot;</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">local_pipeline_package_path</span> <span class=\"o\">=</span> <span class=\"n\">check</span><span class=\"o\">.</span><span class=\"n\">str_param</span><span class=\"p\">(</span>\n            <span class=\"n\">local_pipeline_package_path</span><span class=\"p\">,</span> <span class=\"s2\">&quot;local_pipeline_package_path&quot;</span>\n        <span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">staging_prefix</span> <span class=\"o\">=</span> <span class=\"n\">check</span><span class=\"o\">.</span><span class=\"n\">str_param</span><span class=\"p\">(</span><span class=\"n\">staging_prefix</span><span class=\"p\">,</span> <span class=\"s2\">&quot;staging_prefix&quot;</span><span class=\"p\">)</span>\n        <span class=\"n\">check</span><span class=\"o\">.</span><span class=\"n\">invariant</span><span class=\"p\">(</span><span class=\"n\">staging_prefix</span><span class=\"o\">.</span><span class=\"n\">startswith</span><span class=\"p\">(</span><span class=\"s2\">&quot;/&quot;</span><span class=\"p\">),</span> <span class=\"s2\">&quot;staging_prefix must be an absolute path&quot;</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">wait_for_logs</span> <span class=\"o\">=</span> <span class=\"n\">check</span><span class=\"o\">.</span><span class=\"n\">bool_param</span><span class=\"p\">(</span><span class=\"n\">wait_for_logs</span><span class=\"p\">,</span> <span class=\"s2\">&quot;wait_for_logs&quot;</span><span class=\"p\">)</span>\n\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">databricks_runner</span> <span class=\"o\">=</span> <span class=\"n\">DatabricksJobRunner</span><span class=\"p\">(</span><span class=\"n\">host</span><span class=\"o\">=</span><span class=\"n\">databricks_host</span><span class=\"p\">,</span> <span class=\"n\">token</span><span class=\"o\">=</span><span class=\"n\">databricks_token</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">launch_step</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">step_context</span><span class=\"p\">,</span> <span class=\"n\">prior_attempts_count</span><span class=\"p\">):</span>\n        <span class=\"n\">step_run_ref</span> <span class=\"o\">=</span> <span class=\"n\">step_context_to_step_run_ref</span><span class=\"p\">(</span>\n            <span class=\"n\">step_context</span><span class=\"p\">,</span> <span class=\"n\">prior_attempts_count</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">local_pipeline_package_path</span>\n        <span class=\"p\">)</span>\n        <span class=\"n\">run_id</span> <span class=\"o\">=</span> <span class=\"n\">step_context</span><span class=\"o\">.</span><span class=\"n\">pipeline_run</span><span class=\"o\">.</span><span class=\"n\">run_id</span>\n        <span class=\"n\">log</span> <span class=\"o\">=</span> <span class=\"n\">step_context</span><span class=\"o\">.</span><span class=\"n\">log</span>\n\n        <span class=\"n\">step_key</span> <span class=\"o\">=</span> <span class=\"n\">step_run_ref</span><span class=\"o\">.</span><span class=\"n\">step_key</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_upload_artifacts</span><span class=\"p\">(</span><span class=\"n\">log</span><span class=\"p\">,</span> <span class=\"n\">step_run_ref</span><span class=\"p\">,</span> <span class=\"n\">run_id</span><span class=\"p\">,</span> <span class=\"n\">step_key</span><span class=\"p\">)</span>\n\n        <span class=\"n\">task</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_get_databricks_task</span><span class=\"p\">(</span><span class=\"n\">run_id</span><span class=\"p\">,</span> <span class=\"n\">step_key</span><span class=\"p\">)</span>\n        <span class=\"n\">databricks_run_id</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">databricks_runner</span><span class=\"o\">.</span><span class=\"n\">submit_run</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">run_config</span><span class=\"p\">,</span> <span class=\"n\">task</span><span class=\"p\">)</span>\n\n        <span class=\"k\">try</span><span class=\"p\">:</span>\n            <span class=\"c1\"># If this is being called within a `delay_interrupts` context, allow interrupts while</span>\n            <span class=\"c1\"># waiting for the  execution to complete, so that we can terminate slow or hanging steps</span>\n            <span class=\"k\">with</span> <span class=\"n\">raise_interrupts_immediately</span><span class=\"p\">():</span>\n                <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">databricks_runner</span><span class=\"o\">.</span><span class=\"n\">wait_for_run_to_complete</span><span class=\"p\">(</span><span class=\"n\">log</span><span class=\"p\">,</span> <span class=\"n\">databricks_run_id</span><span class=\"p\">)</span>\n        <span class=\"k\">finally</span><span class=\"p\">:</span>\n            <span class=\"k\">if</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">wait_for_logs</span><span class=\"p\">:</span>\n                <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_log_logs_from_cluster</span><span class=\"p\">(</span><span class=\"n\">log</span><span class=\"p\">,</span> <span class=\"n\">databricks_run_id</span><span class=\"p\">)</span>\n\n        <span class=\"k\">for</span> <span class=\"n\">event</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">get_step_events</span><span class=\"p\">(</span><span class=\"n\">run_id</span><span class=\"p\">,</span> <span class=\"n\">step_key</span><span class=\"p\">):</span>\n            <span class=\"n\">log_step_event</span><span class=\"p\">(</span><span class=\"n\">step_context</span><span class=\"p\">,</span> <span class=\"n\">event</span><span class=\"p\">)</span>\n            <span class=\"k\">yield</span> <span class=\"n\">event</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">get_step_events</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">run_id</span><span class=\"p\">,</span> <span class=\"n\">step_key</span><span class=\"p\">):</span>\n        <span class=\"n\">path</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_dbfs_path</span><span class=\"p\">(</span><span class=\"n\">run_id</span><span class=\"p\">,</span> <span class=\"n\">step_key</span><span class=\"p\">,</span> <span class=\"n\">PICKLED_EVENTS_FILE_NAME</span><span class=\"p\">)</span>\n        <span class=\"n\">events_data</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">databricks_runner</span><span class=\"o\">.</span><span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">read_file</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">deserialize_value</span><span class=\"p\">(</span><span class=\"n\">pickle</span><span class=\"o\">.</span><span class=\"n\">loads</span><span class=\"p\">(</span><span class=\"n\">events_data</span><span class=\"p\">))</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">_get_databricks_task</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">run_id</span><span class=\"p\">,</span> <span class=\"n\">step_key</span><span class=\"p\">):</span>\n        <span class=\"sd\">&quot;&quot;&quot;Construct the &#39;task&#39; parameter to  be submitted to the Databricks API.</span>\n\n<span class=\"sd\">        This will create a &#39;spark_python_task&#39; dict where `python_file` is a path on DBFS</span>\n<span class=\"sd\">        pointing to the &#39;databricks_step_main.py&#39; file, and `parameters` is an array with a single</span>\n<span class=\"sd\">        element, a path on DBFS pointing to the picked `step_run_ref` data.</span>\n\n<span class=\"sd\">        See https://docs.databricks.com/dev-tools/api/latest/jobs.html#jobssparkpythontask.</span>\n<span class=\"sd\">        &quot;&quot;&quot;</span>\n        <span class=\"n\">python_file</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_dbfs_path</span><span class=\"p\">(</span><span class=\"n\">run_id</span><span class=\"p\">,</span> <span class=\"n\">step_key</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_main_file_name</span><span class=\"p\">())</span>\n        <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_internal_dbfs_path</span><span class=\"p\">(</span><span class=\"n\">run_id</span><span class=\"p\">,</span> <span class=\"n\">step_key</span><span class=\"p\">,</span> <span class=\"n\">PICKLED_STEP_RUN_REF_FILE_NAME</span><span class=\"p\">),</span>\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_internal_dbfs_path</span><span class=\"p\">(</span><span class=\"n\">run_id</span><span class=\"p\">,</span> <span class=\"n\">step_key</span><span class=\"p\">,</span> <span class=\"n\">PICKLED_CONFIG_FILE_NAME</span><span class=\"p\">),</span>\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_internal_dbfs_path</span><span class=\"p\">(</span><span class=\"n\">run_id</span><span class=\"p\">,</span> <span class=\"n\">step_key</span><span class=\"p\">,</span> <span class=\"n\">CODE_ZIP_NAME</span><span class=\"p\">),</span>\n        <span class=\"p\">]</span>\n        <span class=\"k\">return</span> <span class=\"p\">{</span><span class=\"s2\">&quot;spark_python_task&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;python_file&quot;</span><span class=\"p\">:</span> <span class=\"n\">python_file</span><span class=\"p\">,</span> <span class=\"s2\">&quot;parameters&quot;</span><span class=\"p\">:</span> <span class=\"n\">parameters</span><span class=\"p\">}}</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">_upload_artifacts</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">log</span><span class=\"p\">,</span> <span class=\"n\">step_run_ref</span><span class=\"p\">,</span> <span class=\"n\">run_id</span><span class=\"p\">,</span> <span class=\"n\">step_key</span><span class=\"p\">):</span>\n        <span class=\"sd\">&quot;&quot;&quot;Upload the step run ref and pyspark code to DBFS to run as a job.&quot;&quot;&quot;</span>\n\n        <span class=\"n\">log</span><span class=\"o\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"s2\">&quot;Uploading main file to DBFS&quot;</span><span class=\"p\">)</span>\n        <span class=\"n\">main_local_path</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_main_file_local_path</span><span class=\"p\">()</span>\n        <span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"n\">main_local_path</span><span class=\"p\">,</span> <span class=\"s2\">&quot;rb&quot;</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">infile</span><span class=\"p\">:</span>\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">databricks_runner</span><span class=\"o\">.</span><span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">put_file</span><span class=\"p\">(</span>\n                <span class=\"n\">infile</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_dbfs_path</span><span class=\"p\">(</span><span class=\"n\">run_id</span><span class=\"p\">,</span> <span class=\"n\">step_key</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_main_file_name</span><span class=\"p\">())</span>\n            <span class=\"p\">)</span>\n\n        <span class=\"n\">log</span><span class=\"o\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"s2\">&quot;Uploading pipeline to DBFS&quot;</span><span class=\"p\">)</span>\n        <span class=\"k\">with</span> <span class=\"n\">seven</span><span class=\"o\">.</span><span class=\"n\">TemporaryDirectory</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">temp_dir</span><span class=\"p\">:</span>\n            <span class=\"c1\"># Zip and upload package containing pipeline</span>\n            <span class=\"n\">zip_local_path</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">temp_dir</span><span class=\"p\">,</span> <span class=\"n\">CODE_ZIP_NAME</span><span class=\"p\">)</span>\n            <span class=\"n\">build_pyspark_zip</span><span class=\"p\">(</span><span class=\"n\">zip_local_path</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">local_pipeline_package_path</span><span class=\"p\">)</span>\n            <span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"n\">zip_local_path</span><span class=\"p\">,</span> <span class=\"s2\">&quot;rb&quot;</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">infile</span><span class=\"p\">:</span>\n                <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">databricks_runner</span><span class=\"o\">.</span><span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">put_file</span><span class=\"p\">(</span>\n                    <span class=\"n\">infile</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_dbfs_path</span><span class=\"p\">(</span><span class=\"n\">run_id</span><span class=\"p\">,</span> <span class=\"n\">step_key</span><span class=\"p\">,</span> <span class=\"n\">CODE_ZIP_NAME</span><span class=\"p\">)</span>\n                <span class=\"p\">)</span>\n\n        <span class=\"n\">log</span><span class=\"o\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"s2\">&quot;Uploading step run ref file to DBFS&quot;</span><span class=\"p\">)</span>\n        <span class=\"n\">step_pickle_file</span> <span class=\"o\">=</span> <span class=\"n\">io</span><span class=\"o\">.</span><span class=\"n\">BytesIO</span><span class=\"p\">()</span>\n\n        <span class=\"n\">pickle</span><span class=\"o\">.</span><span class=\"n\">dump</span><span class=\"p\">(</span><span class=\"n\">step_run_ref</span><span class=\"p\">,</span> <span class=\"n\">step_pickle_file</span><span class=\"p\">)</span>\n        <span class=\"n\">step_pickle_file</span><span class=\"o\">.</span><span class=\"n\">seek</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">databricks_runner</span><span class=\"o\">.</span><span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">put_file</span><span class=\"p\">(</span>\n            <span class=\"n\">step_pickle_file</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_dbfs_path</span><span class=\"p\">(</span><span class=\"n\">run_id</span><span class=\"p\">,</span> <span class=\"n\">step_key</span><span class=\"p\">,</span> <span class=\"n\">PICKLED_STEP_RUN_REF_FILE_NAME</span><span class=\"p\">),</span>\n        <span class=\"p\">)</span>\n\n        <span class=\"n\">databricks_config</span> <span class=\"o\">=</span> <span class=\"n\">DatabricksConfig</span><span class=\"p\">(</span><span class=\"n\">storage</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">storage</span><span class=\"p\">,</span> <span class=\"n\">secrets</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">secrets</span><span class=\"p\">,)</span>\n        <span class=\"n\">log</span><span class=\"o\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"s2\">&quot;Uploading Databricks configuration to DBFS&quot;</span><span class=\"p\">)</span>\n        <span class=\"n\">databricks_config_file</span> <span class=\"o\">=</span> <span class=\"n\">io</span><span class=\"o\">.</span><span class=\"n\">BytesIO</span><span class=\"p\">()</span>\n\n        <span class=\"n\">pickle</span><span class=\"o\">.</span><span class=\"n\">dump</span><span class=\"p\">(</span><span class=\"n\">databricks_config</span><span class=\"p\">,</span> <span class=\"n\">databricks_config_file</span><span class=\"p\">)</span>\n        <span class=\"n\">databricks_config_file</span><span class=\"o\">.</span><span class=\"n\">seek</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">databricks_runner</span><span class=\"o\">.</span><span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">put_file</span><span class=\"p\">(</span>\n            <span class=\"n\">databricks_config_file</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_dbfs_path</span><span class=\"p\">(</span><span class=\"n\">run_id</span><span class=\"p\">,</span> <span class=\"n\">step_key</span><span class=\"p\">,</span> <span class=\"n\">PICKLED_CONFIG_FILE_NAME</span><span class=\"p\">),</span>\n        <span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">_log_logs_from_cluster</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">log</span><span class=\"p\">,</span> <span class=\"n\">run_id</span><span class=\"p\">):</span>\n        <span class=\"n\">logs</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">databricks_runner</span><span class=\"o\">.</span><span class=\"n\">retrieve_logs_for_run_id</span><span class=\"p\">(</span><span class=\"n\">log</span><span class=\"p\">,</span> <span class=\"n\">run_id</span><span class=\"p\">)</span>\n        <span class=\"k\">if</span> <span class=\"n\">logs</span> <span class=\"ow\">is</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n            <span class=\"k\">return</span>\n        <span class=\"n\">stdout</span><span class=\"p\">,</span> <span class=\"n\">stderr</span> <span class=\"o\">=</span> <span class=\"n\">logs</span>\n        <span class=\"k\">if</span> <span class=\"n\">stderr</span><span class=\"p\">:</span>\n            <span class=\"n\">log</span><span class=\"o\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"n\">stderr</span><span class=\"p\">)</span>\n        <span class=\"k\">if</span> <span class=\"n\">stdout</span><span class=\"p\">:</span>\n            <span class=\"n\">log</span><span class=\"o\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"n\">stdout</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">_main_file_name</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">basename</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_main_file_local_path</span><span class=\"p\">())</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">_main_file_local_path</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"n\">databricks_step_main</span><span class=\"o\">.</span><span class=\"vm\">__file__</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">_dbfs_path</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">run_id</span><span class=\"p\">,</span> <span class=\"n\">step_key</span><span class=\"p\">,</span> <span class=\"n\">filename</span><span class=\"p\">):</span>\n        <span class=\"n\">path</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;/&quot;</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">([</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">staging_prefix</span><span class=\"p\">,</span> <span class=\"n\">run_id</span><span class=\"p\">,</span> <span class=\"n\">step_key</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">basename</span><span class=\"p\">(</span><span class=\"n\">filename</span><span class=\"p\">)])</span>\n        <span class=\"k\">return</span> <span class=\"s2\">&quot;dbfs://</span><span class=\"si\">{}</span><span class=\"s2\">&quot;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">_internal_dbfs_path</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">run_id</span><span class=\"p\">,</span> <span class=\"n\">step_key</span><span class=\"p\">,</span> <span class=\"n\">filename</span><span class=\"p\">):</span>\n        <span class=\"sd\">&quot;&quot;&quot;Scripts running on Databricks should access DBFS at /dbfs/.&quot;&quot;&quot;</span>\n        <span class=\"n\">path</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;/&quot;</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">([</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">staging_prefix</span><span class=\"p\">,</span> <span class=\"n\">run_id</span><span class=\"p\">,</span> <span class=\"n\">step_key</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">basename</span><span class=\"p\">(</span><span class=\"n\">filename</span><span class=\"p\">)])</span>\n        <span class=\"k\">return</span> <span class=\"s2\">&quot;/dbfs/</span><span class=\"si\">{}</span><span class=\"s2\">&quot;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"p\">)</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">DatabricksConfig</span><span class=\"p\">:</span>\n    <span class=\"sd\">&quot;&quot;&quot;Represents configuration required by Databricks to run jobs.</span>\n\n<span class=\"sd\">    Instances of this class will be created when a Databricks step is launched and will contain</span>\n<span class=\"sd\">    all configuration and secrets required to set up storage and environment variables within</span>\n<span class=\"sd\">    the Databricks environment. The instance will be serialized and uploaded to Databricks</span>\n<span class=\"sd\">    by the step launcher, then deserialized as part of the &#39;main&#39; script when the job is running</span>\n<span class=\"sd\">    in Databricks.</span>\n\n<span class=\"sd\">    The `setup` method handles the actual setup prior to solid execution on the Databricks side.</span>\n\n<span class=\"sd\">    This config is separated out from the regular Dagster run config system because the setup</span>\n<span class=\"sd\">    is done by the &#39;main&#39; script before entering a Dagster context (i.e. using `run_step_from_ref`).</span>\n<span class=\"sd\">    We use a separate class to avoid coupling the setup to the format of the `step_run_ref` object.</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">storage</span><span class=\"p\">,</span> <span class=\"n\">secrets</span><span class=\"p\">):</span>\n        <span class=\"sd\">&quot;&quot;&quot;Create a new DatabricksConfig object.</span>\n\n<span class=\"sd\">        `storage` and `secrets` should be of the same shape as the `storage` and</span>\n<span class=\"sd\">        `secrets_to_env_variables` config passed to `databricks_pyspark_step_launcher`.</span>\n<span class=\"sd\">        &quot;&quot;&quot;</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">storage</span> <span class=\"o\">=</span> <span class=\"n\">storage</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">secrets</span> <span class=\"o\">=</span> <span class=\"n\">secrets</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">setup</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">dbutils</span><span class=\"p\">,</span> <span class=\"n\">sc</span><span class=\"p\">):</span>\n        <span class=\"sd\">&quot;&quot;&quot;Set up storage and environment variables on Databricks.</span>\n\n<span class=\"sd\">        The `dbutils` and `sc` arguments must be passed in by the &#39;main&#39; script, as they</span>\n<span class=\"sd\">        aren&#39;t accessible by any other modules.</span>\n<span class=\"sd\">        &quot;&quot;&quot;</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">setup_storage</span><span class=\"p\">(</span><span class=\"n\">dbutils</span><span class=\"p\">,</span> <span class=\"n\">sc</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">setup_environment</span><span class=\"p\">(</span><span class=\"n\">dbutils</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">setup_storage</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">dbutils</span><span class=\"p\">,</span> <span class=\"n\">sc</span><span class=\"p\">):</span>\n        <span class=\"sd\">&quot;&quot;&quot;Set up storage using either S3 or ADLS2.&quot;&quot;&quot;</span>\n        <span class=\"k\">if</span> <span class=\"s2\">&quot;s3&quot;</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">storage</span><span class=\"p\">:</span>\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">setup_s3_storage</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">storage</span><span class=\"p\">[</span><span class=\"s2\">&quot;s3&quot;</span><span class=\"p\">],</span> <span class=\"n\">dbutils</span><span class=\"p\">,</span> <span class=\"n\">sc</span><span class=\"p\">)</span>\n        <span class=\"k\">elif</span> <span class=\"s2\">&quot;adls2&quot;</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">storage</span><span class=\"p\">:</span>\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">setup_adls2_storage</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">storage</span><span class=\"p\">[</span><span class=\"s2\">&quot;adls2&quot;</span><span class=\"p\">],</span> <span class=\"n\">dbutils</span><span class=\"p\">,</span> <span class=\"n\">sc</span><span class=\"p\">)</span>\n        <span class=\"k\">else</span><span class=\"p\">:</span>\n            <span class=\"k\">raise</span> <span class=\"ne\">Exception</span><span class=\"p\">(</span><span class=\"s2\">&quot;No valid storage found in Databricks configuration!&quot;</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">setup_s3_storage</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">s3_storage</span><span class=\"p\">,</span> <span class=\"n\">dbutils</span><span class=\"p\">,</span> <span class=\"n\">sc</span><span class=\"p\">):</span>\n        <span class=\"sd\">&quot;&quot;&quot;Obtain AWS credentials from Databricks secrets and export so both Spark and boto can use them.&quot;&quot;&quot;</span>\n\n        <span class=\"n\">scope</span> <span class=\"o\">=</span> <span class=\"n\">s3_storage</span><span class=\"p\">[</span><span class=\"s2\">&quot;secret_scope&quot;</span><span class=\"p\">]</span>\n\n        <span class=\"n\">access_key</span> <span class=\"o\">=</span> <span class=\"n\">dbutils</span><span class=\"o\">.</span><span class=\"n\">secrets</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"n\">scope</span><span class=\"o\">=</span><span class=\"n\">scope</span><span class=\"p\">,</span> <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"n\">s3_storage</span><span class=\"p\">[</span><span class=\"s2\">&quot;access_key_key&quot;</span><span class=\"p\">])</span>\n        <span class=\"n\">secret_key</span> <span class=\"o\">=</span> <span class=\"n\">dbutils</span><span class=\"o\">.</span><span class=\"n\">secrets</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"n\">scope</span><span class=\"o\">=</span><span class=\"n\">scope</span><span class=\"p\">,</span> <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"n\">s3_storage</span><span class=\"p\">[</span><span class=\"s2\">&quot;secret_key_key&quot;</span><span class=\"p\">])</span>\n\n        <span class=\"c1\"># Spark APIs will use this.</span>\n        <span class=\"c1\"># See https://docs.databricks.com/data/data-sources/aws/amazon-s3.html#alternative-1-set-aws-keys-in-the-spark-context.</span>\n        <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">_jsc</span><span class=\"o\">.</span><span class=\"n\">hadoopConfiguration</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">set</span><span class=\"p\">(</span>  <span class=\"c1\"># pylint: disable=protected-access</span>\n            <span class=\"s2\">&quot;fs.s3n.awsAccessKeyId&quot;</span><span class=\"p\">,</span> <span class=\"n\">access_key</span>\n        <span class=\"p\">)</span>\n        <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">_jsc</span><span class=\"o\">.</span><span class=\"n\">hadoopConfiguration</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">set</span><span class=\"p\">(</span>  <span class=\"c1\"># pylint: disable=protected-access</span>\n            <span class=\"s2\">&quot;fs.s3n.awsSecretAccessKey&quot;</span><span class=\"p\">,</span> <span class=\"n\">secret_key</span>\n        <span class=\"p\">)</span>\n\n        <span class=\"c1\"># Boto will use these.</span>\n        <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s2\">&quot;AWS_ACCESS_KEY_ID&quot;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">access_key</span>\n        <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s2\">&quot;AWS_SECRET_ACCESS_KEY&quot;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">secret_key</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">setup_adls2_storage</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">adls2_storage</span><span class=\"p\">,</span> <span class=\"n\">dbutils</span><span class=\"p\">,</span> <span class=\"n\">sc</span><span class=\"p\">):</span>\n        <span class=\"sd\">&quot;&quot;&quot;Obtain an Azure Storage Account key from Databricks secrets and export so Spark can use it.&quot;&quot;&quot;</span>\n        <span class=\"n\">storage_account_key</span> <span class=\"o\">=</span> <span class=\"n\">dbutils</span><span class=\"o\">.</span><span class=\"n\">secrets</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span>\n            <span class=\"n\">scope</span><span class=\"o\">=</span><span class=\"n\">adls2_storage</span><span class=\"p\">[</span><span class=\"s2\">&quot;secret_scope&quot;</span><span class=\"p\">],</span> <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"n\">adls2_storage</span><span class=\"p\">[</span><span class=\"s2\">&quot;storage_account_key_key&quot;</span><span class=\"p\">]</span>\n        <span class=\"p\">)</span>\n        <span class=\"c1\"># Spark APIs will use this.</span>\n        <span class=\"c1\"># See https://docs.microsoft.com/en-gb/azure/databricks/data/data-sources/azure/azure-datalake-gen2#--access-directly-using-the-storage-account-access-key</span>\n        <span class=\"c1\"># sc is globally defined in the Databricks runtime and points to the Spark context</span>\n        <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">_jsc</span><span class=\"o\">.</span><span class=\"n\">hadoopConfiguration</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">set</span><span class=\"p\">(</span>  <span class=\"c1\"># pylint: disable=protected-access</span>\n            <span class=\"s2\">&quot;fs.azure.account.key.</span><span class=\"si\">{}</span><span class=\"s2\">.dfs.core.windows.net&quot;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span>\n                <span class=\"n\">adls2_storage</span><span class=\"p\">[</span><span class=\"s2\">&quot;storage_account_name&quot;</span><span class=\"p\">]</span>\n            <span class=\"p\">),</span>\n            <span class=\"n\">storage_account_key</span><span class=\"p\">,</span>\n        <span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">setup_environment</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">dbutils</span><span class=\"p\">):</span>\n        <span class=\"sd\">&quot;&quot;&quot;Setup any environment variables required by the run.</span>\n\n<span class=\"sd\">        Extract any secrets in the run config and export them as environment variables.</span>\n\n<span class=\"sd\">        This is important for any `StringSource` config since the environment variables</span>\n<span class=\"sd\">        won&#39;t ordinarily be available in the Databricks execution environment.</span>\n<span class=\"sd\">        &quot;&quot;&quot;</span>\n        <span class=\"k\">for</span> <span class=\"n\">secret</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">secrets</span><span class=\"p\">:</span>\n            <span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"n\">secret</span><span class=\"p\">[</span><span class=\"s2\">&quot;name&quot;</span><span class=\"p\">]</span>\n            <span class=\"n\">key</span> <span class=\"o\">=</span> <span class=\"n\">secret</span><span class=\"p\">[</span><span class=\"s2\">&quot;key&quot;</span><span class=\"p\">]</span>\n            <span class=\"n\">scope</span> <span class=\"o\">=</span> <span class=\"n\">secret</span><span class=\"p\">[</span><span class=\"s2\">&quot;scope&quot;</span><span class=\"p\">]</span>\n            <span class=\"nb\">print</span><span class=\"p\">(</span>  <span class=\"c1\"># pylint: disable=print-call</span>\n                <span class=\"s2\">&quot;Exporting </span><span class=\"si\">{}</span><span class=\"s2\"> from Databricks secret </span><span class=\"si\">{}</span><span class=\"s2\">, scope </span><span class=\"si\">{}</span><span class=\"s2\">&quot;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"p\">,</span> <span class=\"n\">key</span><span class=\"p\">,</span> <span class=\"n\">scope</span><span class=\"p\">)</span>\n            <span class=\"p\">)</span>\n            <span class=\"n\">val</span> <span class=\"o\">=</span> <span class=\"n\">dbutils</span><span class=\"o\">.</span><span class=\"n\">secrets</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"n\">scope</span><span class=\"o\">=</span><span class=\"n\">scope</span><span class=\"p\">,</span> <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"n\">key</span><span class=\"p\">)</span>\n            <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"n\">name</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">val</span>\n</pre></div>", "current_page_name": "_modules/dagster_databricks/databricks_pyspark_step_launcher", "sidebars": ["globaltoc.html", "searchbox.html"], "customsidebar": null, "alabaster_version": "0.7.12"}