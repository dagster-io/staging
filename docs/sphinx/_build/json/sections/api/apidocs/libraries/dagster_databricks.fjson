{"parents": [], "prev": {"link": "../dagster_dask/", "title": "Dask (dagster_dask)"}, "next": {"link": "../dagster_datadog/", "title": "Datadog (dagster_datadog)"}, "title": "Databricks (dagster_databricks)", "meta": null, "body": "<div class=\"section\" id=\"databricks-dagster-databricks\">\n<h1>Databricks (dagster_databricks)<a class=\"headerlink\" href=\"#databricks-dagster-databricks\" title=\"Permalink to this headline\">\u00b6</a></h1>\n<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">dagster_databricks</span></code> package provides two main pieces of functionality:</p>\n<ul class=\"simple\">\n<li><p>A resource, <code class=\"docutils literal notranslate\"><span class=\"pre\">databricks_pyspark_step_launcher</span></code>, which will execute a solid within a Databricks\ncontext on a cluster, such that the <code class=\"docutils literal notranslate\"><span class=\"pre\">pyspark</span></code> resource uses the cluster\u2019s Spark instance.</p></li>\n<li><p>A function, <code class=\"docutils literal notranslate\"><span class=\"pre\">create_databricks_job_solid</span></code>, which creates a solid that submits an external\nconfigurable job to Databricks using the \u2018Run Now\u2019 API.</p></li>\n</ul>\n<p>Note that, for the <code class=\"docutils literal notranslate\"><span class=\"pre\">databricks_pyspark_step_launcher</span></code>, either S3 or Azure Data Lake Storage config\n<strong>must</strong> be specified for solids to succeed, and the credentials for this storage must also be\nstored as a Databricks Secret and stored in the resource config so that the Databricks cluster can\naccess storage.</p>\n<dl class=\"function\">\n<dt id=\"dagster_databricks.create_databricks_job_solid\">\n<code class=\"sig-prename descclassname\">dagster_databricks.</code><code class=\"sig-name descname\">create_databricks_job_solid</code><span class=\"sig-paren\">(</span><em class=\"sig-param\">name='databricks_job'</em>, <em class=\"sig-param\">num_inputs=1</em>, <em class=\"sig-param\">description=None</em>, <em class=\"sig-param\">required_resource_keys=frozenset({'databricks_client'})</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../../../../../_modules/dagster_databricks/solids/#create_databricks_job_solid\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#dagster_databricks.create_databricks_job_solid\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Creates a solid that launches a databricks job.</p>\n<p>As config, the solid accepts a blob of the form described in Databricks\u2019 job API:\n<a class=\"reference external\" href=\"https://docs.databricks.com/dev-tools/api/latest/jobs.html\">https://docs.databricks.com/dev-tools/api/latest/jobs.html</a>.</p>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Returns</dt>\n<dd class=\"field-odd\"><p>A solid definition.</p>\n</dd>\n<dt class=\"field-even\">Return type</dt>\n<dd class=\"field-even\"><p><a class=\"reference internal\" href=\"../../solids/#dagster.SolidDefinition\" title=\"dagster.SolidDefinition\">SolidDefinition</a></p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"data\">\n<dt id=\"dagster_databricks.databricks_pyspark_step_launcher\">\n<code class=\"sig-prename descclassname\">dagster_databricks.</code><code class=\"sig-name descname\">databricks_pyspark_step_launcher</code><em class=\"property\"> ResourceDefinition</em><a class=\"reference internal\" href=\"../../../../../_modules/dagster_databricks/databricks_pyspark_step_launcher/#databricks_pyspark_step_launcher\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#dagster_databricks.databricks_pyspark_step_launcher\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Resource for running solids as a Databricks Job.</p>\n<p>When this resource is used, the solid will be executed in Databricks using the \u2018Run Submit\u2019\nAPI. Pipeline code will be zipped up and copied to a directory in DBFS along with the solid\u2019s\nexecution context.</p>\n<p>Use the \u2018run_config\u2019 configuration to specify the details of the Databricks cluster used, and\nthe \u2018storage\u2019 key to configure persistent storage on that cluster. Storage is accessed by\nsetting the credentials in the Spark context, as documented <a class=\"reference external\" href=\"https://docs.databricks.com/data/data-sources/aws/amazon-s3.html#alternative-1-set-aws-keys-in-the-spark-context\">here for S3</a> and <a class=\"reference external\" href=\"https://docs.microsoft.com/en-gb/azure/databricks/data/data-sources/azure/azure-datalake-gen2#--access-directly-using-the-storage-account-access-key\">here for ADLS</a>.</p>\n</dd></dl>\n\n<dl class=\"class\">\n<dt id=\"dagster_databricks.DatabricksError\">\n<em class=\"property\">class </em><code class=\"sig-prename descclassname\">dagster_databricks.</code><code class=\"sig-name descname\">DatabricksError</code><a class=\"reference internal\" href=\"../../../../../_modules/dagster_databricks/databricks/#DatabricksError\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#dagster_databricks.DatabricksError\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd></dd></dl>\n\n</div>\n", "metatags": "", "rellinks": [["genindex", "General Index", "I", "index"], ["py-modindex", "Python Module Index", "", "modules"], ["sections/api/apidocs/libraries/dagster_datadog", "Datadog (dagster_datadog)", "N", "next"], ["sections/api/apidocs/libraries/dagster_dask", "Dask (dagster_dask)", "P", "previous"]], "sourcename": "sections/api/apidocs/libraries/dagster_databricks.rst.txt", "toc": "<ul>\n<li><a class=\"reference internal\" href=\"#\">Databricks (dagster_databricks)</a></li>\n</ul>\n", "display_toc": false, "page_source_suffix": ".rst", "current_page_name": "sections/api/apidocs/libraries/dagster_databricks", "sidebars": ["globaltoc.html", "searchbox.html"], "customsidebar": null, "alabaster_version": "0.7.12"}