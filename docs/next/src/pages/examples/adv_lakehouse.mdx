# adv_lakehouse

Lakehouse is an experimental API built on top of Dagster's core abstractions that makes it easy to
define computations in terms of the data assets that they produce. In the last example, we
demonstrated how to use Lakehouse to transform data assets into a core dagster pipeline. In this
example, we'll be demonstrating how we can use Lakehouse to construct a dagster pipeline with
multiple compute options. Users unfamiliar with lakehouse should first view the first lakehouse
tutorial [here](https://docs.dagster.io/examples/simple_lakehouse).

Different computable assets will often ingest data in different ways. In the earlier example, We had
 a table of temperature samples collected in 5 minute increments, and we wanted to compute the high
 temperature for each day
represented in the table. This pipeline ingested a csv file as a pandas dataframe, and
outputted the computed statistics again as a csv file.
Now, let's say that in addition to the daily high temperature, we wanted to compute the difference
between the high temperature of consecutive days. To do this, we wanted to ingest the original
computed statistics as a Spark Dataframe, and then output back to csv files.
There are a few changes we need to make. First, we would need to change from using single csv files
to utilizing a storage mechanism that is digestible by Spark. In this case, that means representing
tables as _directories_ of csv files as opposed to a single csv file.
From these directories of csv files, we must be able to compute both pandas dataframes and spark
dataframes. How do we utilize both the pandas conversion and the spark conversion?

Luckily, Lakehouse provides a way to _compose_ different types of storage. To see how,
let's first define our storage for converting between a folder of csv files and pandas dataframe.

In the below code examples, note that PandasDF refers to the pandas dataframe class,
`import pandas.Dataframe as PandasDF`, and SparkDF refers to the spark dataframe class,
`import spark.Dataframe as SparkDF`.

## Data Assets

We'll use Assets to define each of the tables again.

```python literalinclude caption=assets.py
file:/adv_lakehouse/adv_lakehouse/assets.py
```

`sfo_q2_weather_sample_table` and `daily_temperature_highs_table` are the same as the earlier
lakehouse example. However, we have an additional table `daily_temperature_high_diffs_table` that
represents the difference between the high temperature of consecutive days. Notice how we use the
`input_assets` parameter to make explicit the dependency on `daily_temperature_highs_table`.

## Storage

The ingestion of csv files is a bit different this time, because our specification requires
ingesting a folder of csv files as opposed to a single csv file.

```python literalinclude caption=lakehouse_def.py
file:/adv_lakehouse/adv_lakehouse/lakehouse_def.py
lines:23-72
```

Notice how the `load` method takes in all csv files within a given directory, rather than specifying
a csv file or set of csv files explicitly. Analogously, the `save` method writes csv files to a
directory in parts.

We'll do something similar for conversion between a csv and a spark datafarame:
```python literalinclude caption=lakehouse_def.py
file:/adv_lakehouse/adv_lakehouse/lakehouse_def.py
lines:75-92
```

To compose these two compute types, we utilize the `multi_type_asset_storage` function
provided by lakehouse:
```python
from lakehouse import multi_type_asset_storage
```

We can now define a composed AssetStorage:

```python literalinclude caption=lakehouse_def.py
file:/adv_lakehouse/adv_lakehouse/lakehouse_def.py
lines:95-98
```

Finally, we construct our lakehouse:

```python literalinclude caption=lakehouse_def.py
file:/adv_lakehouse/adv_lakehouse/lakehouse_def.py
lines:101-110
```

## Pipeline

Once again, using the data assets and storage for handling conversion, we have completely defined
our computation graph.
To construct a pipeline from these assets:
```python literalinclude caption=pipelines.py
file:/adv_lakehouse/adv_lakehouse/pipelines.py
```

Note that the assets don't have to be provided in order. Lakehouse is able to determine asset
ordering by resolving input asset dependencies.




## Open in a playground

[![Open in Gitpod](https://gitpod.io/button/open-in-gitpod.svg)](https://gitpod.io/#EXAMPLE=adv_lakehouse/https://github.com/dagster-io/dagster)

## Download

```
curl https://codeload.github.com/dagster-io/dagster/tar.gz/master | tar -xz --strip=2 dagster-master/examples/adv_lakehouse
cd adv_lakehouse
pip install -e .
```
