import { ExampleReferenceLink } from 'components/ExampleReference';

# Running PySpark code in solids

<ExampleReferenceLink filePath="examples/basic_pyspark" />

Passing PySpark dataframes between solids requires a little bit of extra care, for a cuple reasons:
* Spark has a lazy execution model, which means that PySpark won't process any data until an action like `write` or `collect` is called on a DataFrame.
* PySpark dataframes cannot be pickled, which means that [Asset Stores](/overview/asset-stores/asset_stores) like the `fs_asset_store` won't work for them.

In this example, we've defined an Asset Store that knows how to store and retrieve PySpark dataframes that are produced and consumed by solids.

This example assumes that all the outputs within the pipeline will be PySpark dataframes and stored in the same way.  To learn how to use different Asset Stores for different outputs within the same pipeline, take a look at the [Asset Store overview](/overview/asset-stores/asset_stores).

```python literalinclude caption=repo.py
file:/basic_pyspark/repo.py
startAfter:start_repo_marker_0
endBefore:end_repo_marker_0
```

## Open in a playground

[![Open in Gitpod](https://gitpod.io/button/open-in-gitpod.svg)](https://gitpod.io/#EXAMPLE=basic_pyspark/https://github.com/dagster-io/dagster)

## Download

```
curl https://codeload.github.com/dagster-io/dagster/tar.gz/master | tar -xz --strip=2 dagster-master/examples/basic_pyspark
cd basic_pyspark
```
