import PyObject from 'components/PyObject';
import { DynamicMetaTags } from 'components/MetaTags';

<DynamicMetaTags
  title="Deploying on Kubernetes | Dagster"
  description="A guide to Kubernetes-based deployment of Dagster."
/>

# Deploying on Kubernetes

## Overview

Dagster publishes a [fully-featured Helm chart](https://github.com/dagster-io/dagster/tree/master/helm) 
to manage installing and running a production-grade Kubernetes deployment of Dagster. For each Dagster component in the chart, Dagster
publishes a corresponding Docker image on [DockerHub](https://hub.docker.com/u/dagster).

[Helm](https://helm.sh/) is package manager for Kubernetes applications where users can customize the configuration of created Kubernetes resources via
a [values file or command-line overrides](https://helm.sh/docs/intro/using_helm/#customizing-the-chart-before-installing), both of which
will be used in this guide. References to _values.yaml_ in the following sections refer to [Dagster's values.yaml](https://github.com/dagster-io/dagster/blob/master/helm/dagster/values.yaml).
For users who do not use Helm, this guide is still a useful reference for the components required.

In the _Default Deployment_ section, we go over the simplest way to get started:
* Default Architecture and Components [→](https://docs.dagster.io/deploying/k8s#default-deployment)
* Quickstart: Let's spin up Dagster on K8s! [→](https://docs.dagster.io/deploying/k8s#quickstart)

In the _Production-Grade Deployment_ section, we build off of the _Default Deployment_ and add more sophisticated features including 
solid-level isolation and queuing mechanisms.
* Production-Grade Architecture and Components [→](https://docs.dagster.io/deploying/k8s#production-deployment)
* Helm Configuration Options [→](https://docs.dagster.io/deploying/k8s#configuration)
* Running in Production [→](https://docs.dagster.io/deploying/k8s#production)
* Debugging [→](https://docs.dagster.io/deploying/k8s#debugging)


## Default Deployment

In this section, we cover the simplest deployment option. We walk through the architecture and then describe the Dagster components.

### Default Architecture

<!-- https://excalidraw.com/#json=6263621281644544,Pf5BkEJGb8XU_U4kOTBBEg -->

![k8s_architecture_1.png](/assets/images/deploying/k8s_architecture_1.png)

### Example Walk Through

After a user clicks _Launch Execution_ in Dagit, 

1.  Dagit sends a gRPC request to the User Code Deployment to fetch the current image `company_repository/baz:6` that 
the User Code Deployment is running.
2.  Dagit creates a Pipeline Run entry in the PostgresQL Runs table with `company_repository/baz:6` recorded as the image to be used.
3.  The Daemon checks in a tight loop (<100ms) for all runs that are ready to be executed.
4.  The Daemon finds the Pipeline Run from (2) and spins up a Run Worker, which is an ephemeral K8s Job (with image `company_repository/baz:6`).
5.  The Run Worker traverses the execution plan (executing each solid in the pipeline in topological order) and writes structured events back to PostgreSQL. 

Each of these steps is visible in Dagit.

### Components

####  User Code Deployment: 
Type:  [K8s Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) behind a
  [K8s Service](https://kubernetes.io/docs/concepts/services-networking/service/)

Image:  User-provided (example: [dagster/k8s-example](https://hub.docker.com/r/dagster/k8s-example))

Description:  A User Code Deployment runs a gRPC server and responds to Dagit's requests for
information (such as: "List all of the pipelines in each repository" or "What is the dependency structure of pipeline X?"). 
The user-provided image for the User Code Deployment must contain a 
[repository definition](https://docs.dagster.io/overview/repositories-workspaces/repositories#main) 
and all of the packages needed to execute pipelines / schedules / sensors / etc within the repository.

This component can be updated independently from other Dagster components, including Dagit. Users can 
have multiple User Code Deployments; a common pattern is for each User Code Deployment to correspond to a different repository. 

####  Dagit 
Type:  [K8s Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) behind a
  [K8s Service](https://kubernetes.io/docs/concepts/services-networking/service/)

Image:  [dagster/k8s-dagit](https://hub.docker.com/r/dagster/k8s-dagit) _(released weekly)_ 

The Dagit webserver communicates with the User Code Deployments via gRPC to fetch information needed to populate the Dagit UI. 
Dagit does not load or execute user-written code, adding key robustness to Dagit. Dagit frequently checks whether the User Code Deployment
has been updated; and if so, the new information is fetched.

Dagit can be horizontally scaled by setting the `dagit.replicaCount` field in the _values.yaml_.

####  Daemon (Scheduler)
Type:  [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)

Image:  [dagster/k8s-dagit](https://hub.docker.com/r/dagster/k8s-dagit) _(released weekly)_

The daemon runs in a tight loop (<100ms) and checks the Runs table in PostgreSQL for Pipeline Runs in that are ready to be launched.
The daemon also runs the dagster-native scheduler (LINK!), which has exactly-once guarantees.


####  Run Worker
Type:  [K8s Job](https://kubernetes.io/docs/concepts/workloads/controllers/job/)

Image:  User-provided (same as User Code Deployment)

The Run Worker is responsible for executing the solids in topological order. The image comes from the Pipeline Run entry, which has the image of 
the User Code Deployment at the time the run was requested. The Run Worker uses ephemeral compute, and completes once the run is finished. The 
Run Worker jobs and pods are not automatically deleted so that users are able to inspect results. It is up to the user to delete old jobs and pods 
after noting their status.

####  Database
Type:  PostgreSQL

Image: None or [postgres](https://hub.docker.com/_/postgres)

The user can connect an external database (ie from a cloud provider) or run PostgresQL in a K8s Pod. This
database stores Pipeline Runs, Events, Schedules, etc and powers much of the real-time and historical data visible in Dagit. 
In order to maintain a referenceable history of events, we recommend connecting an external database for most use cases. 

## Quickstart 

The following steps are how to get up and running with the default architecture. This example requires a Kubernetes cluster.

### Step 1: Set up environment
Make sure `kubectl` is configured with the intended Kubernetes cluster.

### Step 2: Build Docker Image for User Code

_Skip this step if using Dagster's example User Code image [dagster/k8s-example](https://hub.docker.com/r/dagster/k8s-example)._

Create a Docker image containing the repository and any dependencies needed to execute the objects in the repository. For reference, here is an example
[Dockerfile](https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation/docker/images/k8s-example/Dockerfile)
and the corresponding [User Code directory](https://github.com/dagster-io/dagster/tree/master/examples/deploy_k8s/example_project). 

### Step :3 Push Docker Image to Registry

_Skip this step if using Dagster's example User Code image._

Publish the image from Step 1 to a registry that is accessible from the Kubernetes cluster, such as AWS ECR or DockerHub.

### Step 4. Add Docker Image to Helm

Within the `userDeployments.deployments` section of _values.yaml_, set `enabled: true` and add your deployment. 

The following snippet works for Dagster's example User Code image.

```yaml
userDeployments:
  enabled: true
  deployments:
    - name: "k8s-example-user-code-1"
      image:
        repository: "dagster/k8s-example"
        tag: latest
        pullPolicy: Always
      dagsterApiGrpcArgs:
        - "-f"
        - "/example_project/example_repo/repo.py"
      port: 3030
```

The field `dagsterApiGrpcArgs` expects a list of arguments for `dagster api grpc` which is run upon Deployment creation and 
starts the gRPC server. To find the applicable arguments, [read more here](https://docs.dagster.io/overview/repositories-workspaces/workspaces#grpc-server). 

### Step 5. Install on Kubernetes cluster

Install the Helm chart and create a "dagster" release:

```shell
helm repo add dagster https://dagster-io.github.io/helm
helm install dagster dagster/dagster -f /path/to/values.yaml --set k8sRunLauncher.enabled=true --set userDeployments.enabled=true --set celery.enabled=false --set rabbitmq.enabled=false
```

To make changes to "dagster" release:

```shell
helm upgrade dagster dagster/dagster -f helm/dagster/values.yaml --set k8sRunLauncher.enabled=true --set userDeployments.enabled=true --set celery.enabled=false --set rabbitmq.enabled=false
```

Helm will launch several pods including PostgreSQL. You can check the status of the installation with `kubectl`. 
If everything worked correctly, you should see output like the following:

```
$ kubectl get pods
NAME                                              READY   STATUS    RESTARTS   AGE
dagster-dagit-645b7d59f8-6lwxh                    1/1     Running   0          11m
dagster-k8s-example-user-code-1-88764b4f4-ds7tn   1/1     Running   0          9m24s
dagster-postgresql-0                              1/1     Running   0          17m
```

### Step 6. Open Dagit and run pipeline!

Start port forwarding to the Dagit pod via:

```shell
export DAGIT_POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=dagster,app.kubernetes.io/instance=dagster,component=dagit" -o jsonpath="{.items[0].metadata.name}")
echo "Visit http://127.0.0.1:8080 to open Dagit"
kubectl --namespace default port-forward $DAGIT_POD_NAME 8080:80
```

Visit http://127.0.0.1:8080, navigate to the [playground](http://127.0.0.1:8080/workspace/example_repo@k8s-example-user-code-1/pipelines/example_pipe/playground), 
select the "default" preset, and click _Launch Execution_.

You can introspect the jobs that were launched with `kubectl`:

```
$ kubectl get jobs
NAME                                               COMPLETIONS   DURATION   AGE
dagster-run-c8f4e3c2-0915-4317-a168-bf8c86810fb2   1/1           4s         6s
```

Within Dagit, you can watch pipeline progress live update and succeed!

## Production-Grade Deployment
In this section, we build off of the _Default Deployment_ and add more sophisticated features.

### Production-Grade Architecture
<!-- https://excalidraw.com/#json=6263621281644544,Pf5BkEJGb8XU_U4kOTBBEg -->

![k8s_architecture_2.png](/assets/images/deploying/k8s_architecture_2.png)

### Example Walk Through

After a user clicks _Launch Execution_ in Dagit, 

1.  [same as default] Dagit sends a gRPC request to the User Code Deployment to fetch the current image `ml_team_repository/baz:6` that 
the User Code Deployment is running.
2.  Dagit creates a Pipeline Run entry with an ENQUEUED state in the the PostgresQL Runs table with `ml_team_repository/baz:6` recorded as the image to be used.
3.  [same as default] The Daemon checks in a tight loop (<100ms) for all runs that are ready to be executed.
4.  The Daemon finds the enqueued pipeline run and then checks whether there is capacity to launch a new run. If so, 
the Daemon creates an ephemeral Run Worker (K8s Job) with image `ml_team_repository/baz:6`.
5.  The Run Worker traverses the execution plan, sending steps that are ready to execute to the configured Celery queue as a Celery task and 
waiting for completion of previously sent steps.
6.  The Celery worker picks tasks off the queue, creates an ephemeral Step Job (K8s Job) with image `ml_team_repository/baz:6` to execute the step,
and waits for completion. (Note how image `ml_team_repository/baz:6` is used for every step associated with the pipeline run.) The Celery Worker sends the result back to the Run Worker, which incorporates the result into the execution plan.
7.  Steps 5 + 6 continue until all executable steps in the Pipeline Run are complete.

Each of these steps is visible in Dagit.

### Components

We'll focus on Celery, Daemon (Run Coordinator portion), and Step Job components. 

We briefly gloss over User Code Deployment and Dagit since these are the same as in the Default Deployment. 

####  Celery
Type:  [K8s Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)

Image:  [dagster/k8s-celery-worker](https://hub.docker.com/r/dagster/k8s-celery-worker) _(released weekly)_

[Celery](https://docs.celeryproject.org/en/stable/) is a distributed task queue that Dagster uses to enable limiting
the number of concurrent connections to a resource. Users can configure multiple Celery queues (for example, one celery queue 
for each resource the user would like to limit) and multiple Celery workers per queue via the `celery` section of _values.yaml_.

The Celery workers poll for new Celery tasks and execute each task in order of receipt or priority. The Celery task largely 
consists of launching an ephemeral Step Job (K8s Job) to execute that step.

Using Celery requires configuring the CeleryK8sRunLauncher and CeleryK8sExecutor ([See Configuration](https://docs.dagster.io/deploying/k8s#configuration)).

####  User Code Deployment
Building off the [prior description](https://docs.dagster.io/deploying/k8s#user-code-deployment), users create a separate 
User Code Deployment with its own image for each repository (which usually corresponds to a team). Users can update any repository 
without causing downtime to any other repository or to Dagit. If there is an error with any repository, an error is surfaced for that 
repository within Dagit; all other repositories and Dagit will still operate normally.

####  Dagit
Building off the [prior description](https://docs.dagster.io/deploying/k8s#dagit), Dagit gracefully handles repository updates and 
errors with no downtime. Dagit is horizontally scalable via `dagit.replicaCount` in _values.yaml_. 

####  Daemon
Run Queueing:
Scheduler:
[Link out to other docs]

The Daemon instigates execution via the <PyObject
module="dagster_celery_k8s" object="CeleryK8sRunLauncher" />, creating a Run Worker
[Job](https://kubernetes.io/docs/concepts/workloads/controllers/job/) with the
user code image specified by the User Code Deployment.

####  Run Worker
Building off the [prior description](https://docs.dagster.io/deploying/k8s#run-worker), the main difference in this deployment is 
that the Run Worker submits steps that are ready to be executed to the corresponding Celery queue (instead of executing
the step itself). As before, the Run Worker is responsible for traversing the execution plan.

#### Step Jobs
Type:  [K8s Job](https://kubernetes.io/docs/concepts/workloads/controllers/job/)
Image: User-provided (same as User Code Deployment)

The Step Job is responsible for executing a single step, writing the structured events to the database. The Celery worker
polls for the Step Job completion.

#### Flower (optional)
Type:  [K8s Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) behind a
  [K8s Service](https://kubernetes.io/docs/concepts/services-networking/service/)

Image:  [mher/flower](https://hub.docker.com/r/mher/flower)

[Flower](https://flower.readthedocs.io/en/latest/) is an optional component that can be useful for monitoring Celery queues
and workers. 

## Set Up Guide

The following steps set up the production-grade architecture. This example requires a Kubernetes cluster and S3 (or other persistent file store).

### Step 1-4 [from Quickstart](http://dagster.io/deploying/k8s#quickstart)
Additionally, configure S3 by 

### Step 5. Configure S3 Secrets

We need to set up S3 or another persistent file store so that data can be passed between steps. To run the Dagster User Code example, 
create a S3 bucket named "dagster-test"; this can be customized in Step 7.

To enable Dagster to connect to S3, provide `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` environment variables via the `env`, `envConfigMaps`, or `envSecrets` 
fields under `userDeployements` in _values.yaml_ or (not recommended) by setting these variables directly in the User Code Deployment image.

### Step 6. Install on Kubernetes cluster

Install the Helm chart and create a "dagster" release:

```shell
helm repo add dagster https://dagster-io.github.io/helm
helm install dagster dagster/dagster -f /path/to/values.yaml --set dagsterDaemon.queuedRunCoordinator.enabled=true --set dagsterDaemon.enabled=true --set k8sRunLauncher.enabled=false --set userDeployments.enabled=true --set celery.enabled=true --set rabbitmq.enabled=true
```

The notable difference in the configuration is
1. Daemon is enabled
2. Celery is enabled
3. K8sRunLauncher is disabled (enabling the CeleryK8sRunLauncher)

To make changes to "dagster" release:

```shell
helm upgrade dagster dagster/dagster -f /path/to/values.yaml --set dagsterDaemon.queuedRunCoordinator.enabled=true --set dagsterDaemon.enabled=true --set k8sRunLauncher.enabled=false --set userDeployments.enabled=true --set celery.enabled=true --set rabbitmq.enabled=true
```

Helm will launch several pods. You can check the status of the installation with `kubectl`. 
If everything worked correctly, you should see output like the following:

```
$ kubectl get pods
NAME                                                     READY   STATUS      RESTARTS   AGE
dagster-celery-workers-74886cfbfb-m9cbc                  1/1     Running     1          3m42s
dagster-daemon-68c4b8d68d-vvpls                          1/1     Running     1          3m42s
dagster-dagit-69974dd75b-5m8gg                           1/1     Running     0          3m42s
dagster-k8s-example-user-code-1-88764b4f4-25mbd          1/1     Running     0          3m42s
dagster-postgresql-0                                     1/1     Running     0          3m42s
dagster-rabbitmq-0                                       1/1     Running     0          3m42s
```

### Step 7. Open Dagit and run pipeline!

Start port forwarding to the Dagit pod via:

```shell
export DAGIT_POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=dagster,app.kubernetes.io/instance=dagster,component=dagit" -o jsonpath="{.items[0].metadata.name}")
echo "Visit http://127.0.0.1:8080 to open Dagit"
kubectl --namespace default port-forward $DAGIT_POD_NAME 8080:80
```

Visit http://127.0.0.1:8080, navigate to the [playground](http://127.0.0.1:8080/workspace/example_repo@k8s-example-user-code-1/pipelines/example_pipe/playground), 
select the "celery_k8s" preset. Notice how `intermediate_storage.s3.config.s3_bucket` is set to `dagster-test`; user can replace this string with 
any other accessible S3 bucket. Then, click _Launch Execution_.

You can introspect the jobs that were launched with `kubectl`:

```
$ kubectl get jobs
NAME                                               COMPLETIONS   DURATION   AGE
dagster-job-9f5c92d1216f636e0d33877560818840       1/1           5s         12s
dagster-job-a1063317b9aac91f42ca9eacec551b6f       1/1           12s        34s
dagster-run-fb6822e5-bf43-476f-9e6c-6f9896cf3fb8   1/1           37s        37s
```

`dagster-job-` entries correspond to Step Jobs and `dagster-run-` entries correspond to Run Workers.

Within Dagit, you can watch pipeline progress live update and succeed!


## Configuration 

We cover some options that are supported out-of-the-box. We encourage users to write custom Run Launcher and Executor if other execution
substrates are used. If you have questions, please reach out to us on [Slack](https://dagster-slackin.herokuapp.com/)!

###  Run Launcher: 
The [Run Launcher](https://docs.dagster.io/overview/run-launchers-executors/run-launcher#main) is responsible for 
allocating the computational resources for run execution.

[K8sRunLauncher](https://docs.dagster.io/_apidocs/libraries/dagster_k8s#dagster_k8s.K8sRunLauncher) [Default]: The K8sRunLauncher
is used in the _Default Deployment_ example. It creates a new K8s Job per pipeline run.
```shell
helm install dagster dagster/dagster -f /path/to/values.yaml --set celery.enabled=false --set k8sRunLauncher.enabled=true
```

[CeleryK8sRunLauncher](https://docs.dagster.io/_apidocs/libraries/dagster_celery_k8s#dagster_celery_k8s.CeleryK8sRunLauncher): 
The CeleryK8sRunLauncher is used in the _Production-Grade Deployment_ example. It creates a new K8s Job per step for step-level isolation and handling concurrency limits.
```shell
helm install dagster dagster/dagster -f /path/to/values.yaml --set celery.enabled=true --set k8sRunLauncher.enabled=false
```

###  Executor:
The Executor traverses the execution plan, enforcing execution order, conditional execution, handling retries, etc.

With the K8sRunLauncher, users can utilize the [In Process Executor](https://docs.dagster.io/_apidocs/execution#dagster.in_process_executor), the 
the [Multiprocess Executor](https://docs.dagster.io/_apidocs/execution#dagster.multiprocess_executor), 
[Celery Executor](https://docs.dagster.io/_apidocs/libraries/dagster_celery#dagster_celery.celery_executor), 
[Celery Docker Executor](https://docs.dagster.io/_apidocs/libraries/dagster_celery_docker#dagster_celery_docker.celery_docker_executor), 
or a custom Executor.

With the CeleryK8sRunLauncher, user can only use the [CeleryK8sJobExecutor](https://docs.dagster.io/_apidocs/libraries/dagster_celery_k8s#dagster_celery_k8s.celery_k8s_job_executor). 
As seen in the "production-grade" example, this creates a Celery task for each Step, allowing for step-level isolation and concurrency control.

###  Scheduler:
Schedule names are generated by hashing the schedule properties, so different repositories can contain schedules with the same
schedule name.

Daemon scheduler [Default]: Uses Dagster-Native scheduler [Need link]. The daemon is also responsible for running sensors (link),
and managing the queue of runs. This is used in both the _Default Deployment_ and _Production-Grade Deployment_ examples and is our recommended
approach.

K8s CronJob scheduler: Uses [Kubernetes CronJob](https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/) directly. 
In order to enable the Dagster K8s CronJob Scheduler, set `scheduler.k8sEnabled` to `true` in the _values.yaml_.

When a new schedule is turned on, Dagster creates a corresponding Kubernetes CronJob object. When a
schedule is updated, Dagster finds and patches the existing CronJob so that there is no downtime. At
execution time, the Kubernetes CronJob will create a Kubernetes Job specifically to instantiate
the Run Launcher, which in turn launches the Run Worker Job.

<PyObject
module="dagster_cron.cron_scheduler" object="SystemCronScheduler" /> [Deprecated]: Uses system cron. Runs in Dagit so limits users to one Dagit server. 
Schedule ticks can be missed if Dagit is unavailable (such as during deploys).


##  Production
Here are some tools and tips for running in production.

### Custom Kubernetes Configuration 
The `dagster-k8s/config` allows users to pass custom configuration to the Kubernetes Job, Job metadata, JobSpec, 
PodSpec, and PodTemplateSpec metadata. Example usage: 

```python
  @solid(
    tags = {
      'dagster-k8s/config': {
        'container_config': {
          'resources': {
            'requests': { 'cpu': '250m', 'memory': '64Mi' },
            'limits': { 'cpu': '500m', 'memory': '2560Mi' },
          }
        },
        'pod_template_spec_metadata': {
          'annotations': { "cluster-autoscaler.kubernetes.io/safe-to-evict": "true"}
        },
        'pod_spec_config': {
          'affinity': {
            'nodeAffinity': {
              'requiredDuringSchedulingIgnoredDuringExecution': {
                'nodeSelectorTerms': [{
                  'matchExpressions': [{
                    'key': 'beta.kubernetes.io/os', 'operator': 'In', 'values': ['windows', 'linux'],
                  }]
                }]
              }
            }
          }
        },
      },
    },
  )
  def my_solid(context):
    context.log.info('running')
```

### Celery Queue Configuration
Users can set “dagster-celery/queue” on solid tags to determine the Celery queue that should be used. 

By default, all solids will be sent to the default Celery queue named "dagster". Example usage: 

```python
  @solid(
    tags = {
      'dagster-celery/queue': 'snowflake_queue',
    }
  )
  def my_solid(context):
    context.log.info('running')
```

### Celery Priority
Users can set “dagster-celery/run_priority” on the pipeline tags to configure the baseline priority of all solids
from that pipeline. Example usage: 

```python
  @solid(
    tags = {
      'dagster-celery/queue': 'snowflake_queue',
    }
  )
  def my_solid(context):
    context.log.info('running')
```

Users can set “dagster-celery/priority” on the solid tags to configure the additional priority of any solid.
“dagster-celery/priority”. Example usage: 

```python
  @solid(
    tags = {
      'dagster-celery/queue': 'snowflake_queue',
    }
  )
  def my_solid(context):
    context.log.info('running')
```

When priorities are set on both the pipeline and solid, the sum of both priorities will be used. 

### Database
In both deployment examples above, we spin up a PostgresQL database within a K8s Pod. In a real deployment,
users will likely want to set up an external PostgreSQL database and configure the `postgres` section of  _values.yaml_ 
(specifically `enabled: false`, `postgresqlHost`, `postgresqlUsername`, `postgresqlPassword`, `postgresqlDatabase`, `service.port`).

### Celery broker 
In both deployment examples above, we spin up RabbitMQ within a K8s Pod. In a real deployment, 
users will likely want to set up an external, like Redis, and configure `rabbitmq` and `redis` sections of _values.yaml_.

To configure an external Redis, set `rabbitmq.enabled: false`, `redis.enabled: false`, `redis.host`, `redis.port`, 
`redis.brokerDbNumber`, `redis.backendDbNumber`.

### Security
Users will likely want to permission a ServiceAccount bound to a properly scoped Role to launch Jobs and create other K8s
resources.

Users will likely want to use [Secrets](https://kubernetes.io/docs/concepts/configuration/secret/) for managing
secure information such as database login.


## Debugging
This section is meant to contain information that does not exactly fit into other sections, but may be useful 
during the debugging process. This section is still a work in progress, and we plan to add to it based on user questions.

To debug why the Run Workers / Step Jobs are using the wrong image, one approach is to check the 
`DAGSTER_CURRENT_IMAGE` environment variable in the User Code Deployment pods and confirm that it is what you expect.
Every time the User Code Deployment is re-deployed, we set the `DAGSTER_CURRENT_IMAGE` environment variable with the image that is 
currently running. It should consist of a simple string in the form `<image_respository>:<image_tag>`. If the image returned
is None, check that the User Code Deployment is up and running.

Overview of the [ConfigMaps](https://kubernetes.io/docs/concepts/configuration/configmap/) generated by the Helm chart.
  - `dagster-instance`: Defines the [Instance YAML](/overview/instances/dagster-instance) for all
    nodes in the system. Configures Dagster storages to use PostgreSQL, schedules to use cron, and
    sets the run launcher as <PyObject module="dagster_celery_k8s" object="CeleryK8sRunLauncher" />
    to launch pipeline runs as Kubernetes
    [Jobs](https://kubernetes.io/docs/concepts/workloads/controllers/job/).
  - `dagster-*-env`: Environment variables for Dagit, the Celery workers, and pipeline execution.
  - `dagster-celery`: Configures the backend/broker which the Celery workers connect to.

As always, we welcome users to reach out to us on [Slack](https://dagster-slackin.herokuapp.com/).
