import PyObject from 'components/PyObject';
import { DynamicMetaTags } from 'components/MetaTags';

<DynamicMetaTags
  title="Deploying on Kubernetes Part 2 | Dagster"
  description="Dagster is a system for building modern data applications."
/>

In this section, we will discuss more sophisticated deployment options for the Dagster system in
Kubernetes. We will focus primarily on 1. the Dagster K8sScheduler, a new scheduler implementation
built on [Kubernetes CronJob](https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/)
and 2. “User Code Deployments”, the ability to load repository information from user code images.

## System Diagram for User Code Deployment with K8s-native Scheduler

<!-- https://excalidraw.com/#json=5698037791850496,se22S4jaW_cPlGhKEzxClQ -->

![k8s_deployment_part2.png](/assets/images/deploying/k8s_deployment_part2.png) #TODO

## K8s Scheduler
This section introduces our integration with Kubernetes CronJob to handle scheduling recurring
pipeline runs.

### Motivation:
Previously, the only scheduler option was the Dagster <PyObject
module="dagster_cron.cron_scheduler" object="SystemCronScheduler" />, which is built on top of
crontab. This ran in the Dagit deployment, which meant that users were restricted to only having
one Dagit deployment and could miss schedule ticks while Dagit was unavailable (such as during
re-deployment).

The motivation for adding the Dagster K8sScheduler implementation is to enable users to have
multiple Dagit deployments without the risk of duplicate pipeline runs or missing pipeline runs.
We increase the robustness of the scheduler by leveraging native Kubernetes objects.

### How to enable:
In order to enable the Dagster K8sScheduler, set scheduler.k8sEnabled to true in the Helm
values.yaml file and fill in the other fields in the scheduler section.

### How it works:
When a new schedule is turned on, we will create a corresponding Kubernetes CronJob object. When a
schedule is updated, we will find and patch the existing CronJob so that there is no downtime. At
execution time, the Kubernetes CronJob will create a new Kubernetes Job specifically to instantiate
the Run Launcher, which in turn creates the Run Coordinator Job.  Kubernetes CronJob names are generated by
hashing the schedule properties, so different repositories can create schedules that have the same
schedule name without causing conflicts in Kubernetes.

## K8s User Code Deployments (over GRPC)

In the previous section, we presented a deployment option where Dagit, Run Coordinator, and
Step Job all use the same image.  This section introduces the option to use a different Docker
image for each repository of user code.

### Motivation:
Previously, we deployed the same image for every component which meant that updating some user code
meant having to redeploy the entire system. Additionally, this meant that the dependencies of
Dagit/Dagster and the dependencies for every user code repository, were inter-mixed together.

With the new system, changes to user code only requires updating the corresponding user deployments
(ie foo and fiz in the diagram above). This enables Dagit to be a long-standing process that
communicates with the user deployments (ie foo/ fiz) via the GRPC layer to fetch the latest
repository information.

Users can create independent Docker images per repository which allows different teams within an
organization to manage their own images and reduce dependencies. This also unlocks the ability to
have different (and conflicting) dependencies.

### How to enable:
Configure the user code deployments by setting userDeployments.enabled to true in the Helm
values.yaml and specify a list of deployments under `userDeployments.deployments`.

### How it works:
This feature operates over GRPC.  First, Dagit will communicate with the user code deployments over
GRPC to populate its UI.  Second, the Run Launcher will communicate with the user code deployments
over GRPC to fetch the most recently deployed image to use in pipeline runs.

We identify the most recently deployed image in the DAGSTER_CURRENT_IMAGE environment variable in
the user code deployment, which is updated at every deploy and consists of a simple string
containing “respository:tag”. When using the <PyObject
module="dagster_celery_k8s" object="CeleryK8sRunLauncher" />, all steps in the pipeline run
will use the same “respository:tag”. As such, we recommend using a unique tag per deployment to
guarantee that each step of a pipeline run uses a Docker image with the same hash.
